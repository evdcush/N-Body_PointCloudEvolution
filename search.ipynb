{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, code, sys, time\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L \n",
    "import chainer.optimizers as optimizers\n",
    "from chainer import cuda\n",
    "import cupy\n",
    "from IPython import display\n",
    "from data_utils import load_data, normalize, to_variable\n",
    "import models, nn, graph_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xp: <module 'cupy' from '/home/evan/.pyenv/versions/3.6.3/envs/P363/lib/python3.6/site-packages/cupy/__init__.py'>\n",
      "rng_seed = 98765 \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Set array backend, seed\n",
    "'''\n",
    "def seed_rng(s):\n",
    "    xp.random.seed(s)\n",
    "    np.random.seed(s)\n",
    "rng_seed = 98765\n",
    "use_gpu = True\n",
    "xp = cupy if use_gpu >= 0 else np\n",
    "print('xp: {}'.format(xp))\n",
    "print('rng_seed = {} '.format(rng_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_minibatch(in_list,batch_size):\n",
    "    if all(len(i) == len(in_list[0]) for i in in_list) == False:   \n",
    "        raise ValueError('Inputs do not have the same dimension')\n",
    "    index_list = np.random.permutation(len(in_list[0]))[:batch_size]#np.random.randint(len(in_list[0]), size=batch_size)\n",
    "    out = list()\n",
    "    rands = np.random.rand(6)\n",
    "    shift = np.random.rand(batch_size,3)\n",
    "    for k in range(len(in_list)):\n",
    "        tmp = in_list[k][index_list]\n",
    "        if rands[0] < .5:\n",
    "            tmp = tmp[:,:,[1,0,2,4,3,5]]\n",
    "        if rands[1] < .5:\n",
    "            tmp = tmp[:,:, [0,2,1,3,5,4]]\n",
    "        if rands[2] < .5:\n",
    "            tmp = tmp[:,:, [2,1,0,5,4,3]]\n",
    "        if rands[3] < .5:\n",
    "            tmp[:,:,0] = 1 - tmp[:,:,0]\n",
    "            tmp[:,:,3] = -tmp[:,:,3]\n",
    "        if rands[4] < .5:\n",
    "            tmp[:,:,1] = 1 - tmp[:,:,1]\n",
    "            tmp[:,:,4] = -tmp[:,:,4]\n",
    "        if rands[5] < .5:\n",
    "            tmp[:,:,2] = 1 - tmp[:,:,2]\n",
    "            tmp[:,:,5] = -tmp[:,:,5]\n",
    "            \n",
    "        tmploc = tmp[:,:,:3]\n",
    "        tmploc += shift[:,None,:]\n",
    "        gt1 = tmploc > 1\n",
    "        tmploc[gt1] = tmploc[gt1] - 1\n",
    "        tmp[:,:,:3] = tmploc\n",
    "        out.append(tmp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "| n_NN | (0.4, 0.0) | (6.0, 0.0) | (4.0, 0.6) | (0.8, 0.2) | (1.0, 0.4) | (2.0, 0.8) | (6.0, 1.5) | (1.2, 0.6) | (2.0, 1.5) | (0.4, 0.2) |\n",
      "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|  0   | 0.0001158  | 0.0024041  | 0.0008920  | 0.0002067  | 0.0001298  | 0.0002501  | 0.0000939  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  8   | 0.0001034  | 0.0054052  | 0.0008854  | 0.0001725  | 0.0001162  | 0.0002425  | 0.0001015  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  9   | 0.0001078  | 0.0056196  | 0.0008831  | 0.0001664  | 0.0001231  | 0.0002438  | 0.0001018  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  10  | 0.0000989  | 0.0054546  | 0.0008849  | 0.0001707  | 0.0001133  | 0.0002428  | 0.0001012  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  11  | 0.0001000  | 0.0057663  | 0.0008843  | 0.0001668  | 0.0001104  | 0.0002421  | 0.0001017  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  12  | 0.0000987  | 0.0056115  | 0.0008838  | 0.0001641  | 0.0001116  | 0.0002419  | 0.0001023  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  13  | 0.0000987  | 0.0055423  | 0.0008840  | 0.0001625  | 0.0001103  | 0.0002422  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  14  | 0.0000966  | 0.0051812  | 0.0008810  | 0.0001635  | 0.0001098  | 0.0002423  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  15  | 0.0000993  | 0.0055656  | 0.0008842  | 0.0001650  | 0.0001103  | 0.0002416  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  16  | 0.0000995  | 0.0050437  | 0.0008834  | 0.0001630  | 0.0001084  | 0.0002408  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  17  | 0.0000970  | 0.0054384  | 0.0008881  | 0.0001614  | 0.0001089  | 0.0002413  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  18  | 0.0000991  | 0.0049268  | 0.0008860  | 0.0001628  | 0.0001093  | 0.0002402  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  19  | 0.0000958  | 0.0057790  | 0.0008847  | 0.0001618  | 0.0001095  | 0.0002415  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  20  | 0.0000967  | 0.0055592  | 0.0008857  | 0.0001614  | 0.0001085  | 0.0002390  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  21  | 0.0001002  | 0.0050618  | 0.0008835  | 0.0001620  | 0.0001099  | 0.0002396  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  22  | 0.0000968  | 0.0054904  | 0.0008857  | 0.0001642  | 0.0001095  | 0.0002451  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  23  | 0.0000963  | 0.0052469  | 0.0008850  | 0.0001649  | 0.0001081  | 0.0002396  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  24  | 0.0000967  | 0.0054465  | 0.0008865  | 0.0001634  | 0.0001079  | 0.0002392  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  25  | 0.0000975  | 0.0049440  | 0.0008845  | 0.0001634  | 0.0001078  | 0.0002431  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  26  | 0.0000970  | 0.0048455  | 0.0008832  | 0.0001631  | 0.0001070  | 0.0002400  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  27  | 0.0000980  | 0.0046936  | 0.0008861  | 0.0001637  | 0.0001081  | 0.0002413  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  28  | 0.0001015  | 0.0047376  | 0.0008850  | 0.0001634  | 0.0001078  | 0.0002425  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  29  | 0.0000984  | 0.0049704  | 0.0008831  | 0.0001632  | 0.0001078  | 0.0002407  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "|  30  | 0.0000985  | 0.0051633  | 0.0008852  | 0.0001623  | 0.0001080  | 0.0002415  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
      "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "param: 13, zX, zY: (6.0, 1.5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-28390f5eda1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fun_bounded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.095\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/envs/P363/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_grad, enable_double_backprop)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \"\"\"\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enable_backprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_double_backprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/envs/P363/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36m_backward_main\u001b[0;34m(self, retain_grad)\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                 \u001b[0mx_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                     \u001b[0mx_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/envs/P363/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         var = Variable(self.data, name=self.name,\n\u001b[0;32m--> 308\u001b[0;31m                        requires_grad=self._requires_grad)\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/envs/P363/lib/python3.6/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             = argument.parse_kwargs(\n\u001b[1;32m    424\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 ('requires_grad', True))\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         if (data is not None and\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/envs/P363/lib/python3.6/site-packages/chainer/utils/argument.py\u001b[0m in \u001b[0;36mparse_kwargs\u001b[0;34m(kwargs, *name_and_values)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mname_and_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     values = [kwargs.pop(name, default_value)\n\u001b[0m\u001b[1;32m      9\u001b[0m               for name, default_value in name_and_values]\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Hyperparameter search\n",
    "'''\n",
    "from IPython.display import clear_output\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "num_particles = 16 # defaults 16**3\n",
    "lr = 0.01\n",
    "mb_size = 8\n",
    "n_epochs = 2500\n",
    "chan_vals = [2**i for i in range(2,9)]\n",
    "chan_depth = [i for i in range(2,15)]\n",
    "#chans = [np.random.choice(chan_vals, np.random.choice(chan_depth)) for i in range(1000)]\n",
    "#bounds = [0.0, 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01]\n",
    "#redshifts = [(0.4, 0.0), (0.8, 0.0), (6.0, 1.2), (4.0, 2.0), (0.8, 0.6), (1.5, 1.0), (1.2, 0.2), (6.0, 0.4)]\n",
    "n_NN = list(np.arange(8,31))\n",
    "n_NN.insert(0,0)\n",
    "redshifts = [(0.4, 0.0), (6.0, 0.0), (4.0, 0.6), (0.8, 0.2), (1.0, 0.4), (2.0, 0.8), (6.0, 1.5), (1.2, 0.6), (2.0, 1.5), (0.4, 0.2)]\n",
    "search_params = n_NN\n",
    "param_label = \"n_NN\"\n",
    "\n",
    "def pprint_avgs(arr):\n",
    "    #lst = [list(row) for row in list(arr)]\n",
    "    lst = [['%0.7f'%elem for elem in row] for row in arr]\n",
    "    ptab = PrettyTable()\n",
    "    ptab.field_names = [param_label] + [str(rs) for rs in redshifts]\n",
    "    for idx, row in enumerate(lst):\n",
    "        param = search_params[idx]\n",
    "        ptab.add_row([param] + row)\n",
    "    print(ptab)\n",
    "\n",
    "#specs = [('knn', 9),('knn', 11),('knn', 13),('knn', 15),('knn', 17),('knn', 21),('knn', 23),('knn', 25),('knn', 27), ]\n",
    "loss_history = np.zeros((n_epochs,len(search_params),len(redshifts)), dtype=np.float32)\n",
    "loss_avgs = np.zeros((len(search_params), len(redshifts)))\n",
    "\n",
    "for rs_idx, rs in enumerate(redshifts):\n",
    "    zX, zY = rs\n",
    "    _X,_Y = load_data(zX, zY, num_particles)\n",
    "    for p_idx, PARAM in enumerate(search_params):\n",
    "        # MUST SEED BEFORE EACH TRAINING SESSION\n",
    "        seed_rng(rng_seed) # insure that every model begins with same\n",
    "        #zX, zY = rs\n",
    "        #_X,_Y = load_data(zX, zY, num_particles)\n",
    "        X,Y = np.copy(_X), np.copy(_Y)\n",
    "        X = normalize(X)\n",
    "        Y = normalize(Y)\n",
    "        chan = [6, 8, 16, 32, 16, 8, 3, 8, 16, 32, 16, 8, 3]\n",
    "        if PARAM == 0.0:\n",
    "            model = models.nBodyModel(chan)\n",
    "        else:\n",
    "            model = models.nBodyModel(chan, ('knn', PARAM))\n",
    "        model.to_gpu()\n",
    "        optimizer = optimizers.Adam(alpha=lr)\n",
    "        optimizer.setup(model)\n",
    "        \n",
    "        cur_settings = [PARAM, rs]\n",
    "        print('param: {}, zX, zY: {}'.format(PARAM, (zX,zY)))\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            model.zerograds()\n",
    "            # data\n",
    "            _x_in_, _x_true_ = next_minibatch([X,Y], mb_size)\n",
    "            _x_in, _x_true = np.copy(_x_in_), np.copy(_x_true_)\n",
    "            x_in = chainer.Variable(cuda.to_gpu(_x_in.astype(xp.float32)))\n",
    "            x_true = chainer.Variable(cuda.to_gpu(_x_true.astype(xp.float32)))\n",
    "            # forward\n",
    "            x_hat = model(x_in, add=True, bounded=True) # prediction\n",
    "            loss = nn.loss_fun_bounded(x_hat, x_true, 0.095)\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            loss_history[epoch-1, p_idx, rs_idx] = cuda.to_cpu(loss.data)\n",
    "        endloss = np.mean(loss_history[-250:, p_idx, rs_idx])\n",
    "        loss_avgs[p_idx, rs_idx] = endloss\n",
    "        rsf = (endloss, cur_settings)\n",
    "        clear_output(wait=True)\n",
    "        pprint_avgs(loss_avgs)\n",
    "           \n",
    "'''\n",
    "+--------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
    "| bounds |     (0.4, 0.0)    |     (0.8, 0.0)    |     (6.0, 1.2)    |     (4.0, 2.0)    |     (0.8, 0.6)    |     (1.5, 1.0)    |     (1.2, 0.2)    |\n",
    "+--------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
    "|  0.0   | 0.000119890377391 | 0.000457710237242 | 0.000162970478414 | 1.83323718375e-05 | 3.16315745295e-05 | 3.01272466459e-05 | 0.000394351867726 |\n",
    "|  0.1   | 0.000115659378935 | 0.000500292750075 | 0.000203645016882 | 1.69450759131e-05 | 9.24624055187e-06 |  2.8423832191e-05 | 0.000487153010909 |\n",
    "|  0.09  | 0.000115799157356 | 0.000559969630558 |  0.00023882460664 | 1.74512952071e-05 | 9.26273605728e-06 | 2.84684101644e-05 | 0.000572036020458 |\n",
    "|  0.08  | 0.000116949391668 | 0.000655652140267 | 0.000299750536215 | 1.96377641259e-05 | 9.30869464355e-06 | 2.90543866868e-05 | 0.000710388296284 |\n",
    "|  0.07  | 0.000120661708934 |  0.00084223295562 | 0.000426335580414 | 2.46218041866e-05 | 9.20692764339e-06 | 3.07395057462e-05 | 0.000908661459107 |\n",
    "|  0.06  | 0.000139344963827 |  0.00122337543871 | 0.000728332379367 | 3.16597470373e-05 | 9.24199503061e-06 | 3.68279579561e-05 |  0.00116449513007 |\n",
    "|  0.05  | 0.000196880107978 |  0.00139848690014 |  0.00132458866574 | 4.45803343609e-05 | 3.16621590173e-05 | 5.26102048752e-05 |  0.0013643016573  |\n",
    "|  0.04  | 0.000389787834138 |  0.00283175101504 |  0.00187213870231 | 6.82938407408e-05 | 3.19446407957e-05 | 9.32425173232e-05 |  0.00223632133566 |\n",
    "|  0.03  |  0.00107194168959 |  0.00341849774122 |  0.00439322041348 | 0.000150620136992 | 3.74845330953e-05 | 0.000194006090169 |  0.00389076955616 |\n",
    "|  0.01  |  0.00541484635323 |  0.0116537790745  |  0.00976342987269 |  0.00506438920274 | 0.000540107314009 |  0.00248598889448 |  0.0114512173459  |\n",
    "| 0.009  |  0.00684926938266 |  0.0126220276579  |   0.017486480996  |  0.00599138485268 | 0.000731166161131 |  0.0032100409735  |  0.0155507419258  |\n",
    "+--------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
    "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
    "| n_NN | (0.4, 0.0) | (6.0, 0.0) | (4.0, 0.6) | (0.8, 0.2) | (1.0, 0.4) | (2.0, 0.8) | (6.0, 1.5) | (1.2, 0.6) | (2.0, 1.5) | (0.4, 0.2) |\n",
    "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
    "|  8   | 0.0001029  | 0.0054619  | 0.0008854  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  9   | 0.0001110  | 0.0054512  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  10  | 0.0000974  | 0.0051782  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  11  | 0.0000966  | 0.0055355  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  12  | 0.0000998  | 0.0055162  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  13  | 0.0000977  | 0.0056525  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  14  | 0.0000968  | 0.0056713  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  15  | 0.0000982  | 0.0054596  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  16  | 0.0000995  | 0.0050161  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  17  | 0.0000974  | 0.0054068  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  18  | 0.0000987  | 0.0049583  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  19  | 0.0000972  | 0.0050308  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  20  | 0.0000970  | 0.0055209  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  21  | 0.0001003  | 0.0051587  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  22  | 0.0000975  | 0.0050356  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  23  | 0.0000959  | 0.0052206  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
    "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
    "| n_NN | (0.4, 0.0) | (6.0, 0.0) | (4.0, 0.6) | (0.8, 0.2) | (1.0, 0.4) | (2.0, 0.8) | (6.0, 1.5) | (1.2, 0.6) | (2.0, 1.5) | (0.4, 0.2) |\n",
    "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
    "|  0   | 0.0001158  | 0.0024041  | 0.0008920  | 0.0002067  | 0.0001298  | 0.0002501  | 0.0000939  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  8   | 0.0001034  | 0.0054052  | 0.0008854  | 0.0001725  | 0.0001162  | 0.0002425  | 0.0001015  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  9   | 0.0001078  | 0.0056196  | 0.0008831  | 0.0001664  | 0.0001231  | 0.0002438  | 0.0001018  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  10  | 0.0000989  | 0.0054546  | 0.0008849  | 0.0001707  | 0.0001133  | 0.0002428  | 0.0001012  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  11  | 0.0001000  | 0.0057663  | 0.0008843  | 0.0001668  | 0.0001104  | 0.0002421  | 0.0001017  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  12  | 0.0000987  | 0.0056115  | 0.0008838  | 0.0001641  | 0.0001116  | 0.0002419  | 0.0001023  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  13  | 0.0000987  | 0.0055423  | 0.0008840  | 0.0001625  | 0.0001103  | 0.0002422  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  14  | 0.0000966  | 0.0051812  | 0.0008810  | 0.0001635  | 0.0001098  | 0.0002423  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  15  | 0.0000993  | 0.0055656  | 0.0008842  | 0.0001650  | 0.0001103  | 0.0002416  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  16  | 0.0000995  | 0.0050437  | 0.0008834  | 0.0001630  | 0.0001084  | 0.0002408  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  17  | 0.0000970  | 0.0054384  | 0.0008881  | 0.0001614  | 0.0001089  | 0.0002413  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  18  | 0.0000991  | 0.0049268  | 0.0008860  | 0.0001628  | 0.0001093  | 0.0002402  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  19  | 0.0000958  | 0.0057790  | 0.0008847  | 0.0001618  | 0.0001095  | 0.0002415  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  20  | 0.0000967  | 0.0055592  | 0.0008857  | 0.0001614  | 0.0001085  | 0.0002390  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  21  | 0.0001002  | 0.0050618  | 0.0008835  | 0.0001620  | 0.0001099  | 0.0002396  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  22  | 0.0000968  | 0.0054904  | 0.0008857  | 0.0001642  | 0.0001095  | 0.0002451  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  23  | 0.0000963  | 0.0052469  | 0.0008850  | 0.0001649  | 0.0001081  | 0.0002396  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  24  | 0.0000967  | 0.0054465  | 0.0008865  | 0.0001634  | 0.0001079  | 0.0002392  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  25  | 0.0000975  | 0.0049440  | 0.0008845  | 0.0001634  | 0.0001078  | 0.0002431  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  26  | 0.0000970  | 0.0048455  | 0.0008832  | 0.0001631  | 0.0001070  | 0.0002400  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  27  | 0.0000980  | 0.0046936  | 0.0008861  | 0.0001637  | 0.0001081  | 0.0002413  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  28  | 0.0001015  | 0.0047376  | 0.0008850  | 0.0001634  | 0.0001078  | 0.0002425  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  29  | 0.0000984  | 0.0049704  | 0.0008831  | 0.0001632  | 0.0001078  | 0.0002407  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "|  30  | 0.0000985  | 0.0051633  | 0.0008852  | 0.0001623  | 0.0001080  | 0.0002415  | 0.0000000  | 0.0000000  | 0.0000000  | 0.0000000  |\n",
    "+------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
    "'''\n",
    "cat = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_by_bound = np.mean(loss_avgs, axis=1)\n",
    "avgs_by_redshift = np.mean(loss_avgs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bounds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-816191a05bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgs_by_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bounds' is not defined"
     ]
    }
   ],
   "source": [
    "bounds[np.argmin(avgs_by_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb6023b470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avgs = avgs_by_bound\n",
    "plt.clf()\n",
    "plt.plot(avgs)\n",
    "plt.stem(avgs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

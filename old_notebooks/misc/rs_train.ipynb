{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "import chainer.optimizers as optimizers\n",
    "from chainer import cuda\n",
    "import numpy as np\n",
    "import cupy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import models\n",
    "import nn\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# experimental models\n",
    "#=============================================================================\n",
    "\n",
    "class RSModel(chainer.Chain):\n",
    "    BOUND = (0.095, 1-0.095)\n",
    "    def __init__(self, channels, *args, layer=models.SetModel, theta=None):\n",
    "        self.channels = channels\n",
    "        last_channels = channels[:-1] + [3,]\n",
    "        self.tags = ['6040', '4020', '2015', '1512', '1210', '1008', '0806', '0604', '0402', '0200']\n",
    "        super(RSModel, self).__init__()        \n",
    "        for i in range(len(self.tags)):\n",
    "            cur_tag = self.tags[i]\n",
    "            channels = self.channels if i != len(self.tags)-1 else last_channels\n",
    "            self.add_link('RS_' + cur_tag, layer(channels))\n",
    "        \n",
    "    def fwd_pred(self, x, rs_tup=(0,10)):\n",
    "        \"\"\"\n",
    "        # each layer receives previoous layer prediction        \n",
    "        rs_tup is idx of redshift\n",
    "        if rs_tup == (7, 10), then you are making prediction from redshift 0.6 to 0.0\n",
    "        \"\"\"\n",
    "        # x.shape == (1, n_P, 6)\n",
    "        rs_start, rs_target = rs_tup\n",
    "        redshift_distance = rs_target - rs_start\n",
    "        assert redshift_distance > 0 and rs_target <= 10\n",
    "        cur_layer = getattr(self, 'RS_' + self.tags[rs_start])\n",
    "        hat = cur_layer(x)\n",
    "        if redshift_distance == 1:\n",
    "            return hat\n",
    "        else:\n",
    "            for i in range(rs_start+1, rs_target):\n",
    "                cur_layer = getattr(self, 'RS_' + self.tags[i])\n",
    "                hat = cur_layer(hat)\n",
    "            return hat\n",
    "    \n",
    "    def fwd_input(self, x):\n",
    "        \"\"\" each layer receives external input\n",
    "        \"\"\"\n",
    "        hat = self.RS_6040(x[0])\n",
    "        error = nn.get_bounded_MSE(hat, x[1], boundary=self.BOUND)\n",
    "        for i in range(1, len(self.tags)):\n",
    "            cur_layer = getattr(self, 'RS_' + self.tags[i])\n",
    "            hat = cur_layer(x[i])\n",
    "            error += nn.get_bounded_MSE(hat, x[i+1], boundary=self.BOUND)\n",
    "        return hat, error\n",
    "    \n",
    "    def fwd_pred_loss(self, x):\n",
    "        \"\"\" each layer receives external input\n",
    "        \"\"\"\n",
    "        hat = self.RS_6040(x[0])\n",
    "        error = nn.get_bounded_MSE(hat, x[1], boundary=self.BOUND)\n",
    "        for i in range(1, len(self.tags)):\n",
    "            cur_layer = getattr(self, 'RS_' + self.tags[i])\n",
    "            hat = cur_layer(hat)\n",
    "            error += nn.get_bounded_MSE(hat, x[i+1], boundary=self.BOUND)\n",
    "        return hat, error\n",
    "    \n",
    "    def fwd_input_sched(self, x, threshold=0.95):\n",
    "        \"\"\" each layer receives external input\n",
    "        \"\"\"\n",
    "        rands = self.xp.random.rand(len(self.tags))\n",
    "        hat = self.RS_6040(x[0])\n",
    "        error = nn.get_bounded_MSE(hat, x[1], boundary=self.BOUND)\n",
    "        for i in range(1, len(self.tags)):\n",
    "            cur_layer = getattr(self, 'RS_' + self.tags[i])\n",
    "            x_in = hat if rands[i] > threshold else x[i]\n",
    "            hat = cur_layer(x_in)\n",
    "            error += nn.get_bounded_MSE(hat, x[i+1], boundary=self.BOUND)\n",
    "        return hat, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static vars\n",
    "#RNG_SEEDS     = [98765, 12345, 319, 77743196] # takes too long\n",
    "#RNG_SEEDS     = [98765, 12345, 77743196] \n",
    "RNG_SEED = 77743196\n",
    "BOUND         = 0.095\n",
    "LEARNING_RATE = 0.01\n",
    "GRAPH_CHANNELS = [6, 8, 16, 32, 16, 8, 3, 8, 16, 32, 16, 8, 3] # for graph model\n",
    "SET_CHANNELS   = [6, 32, 128, 256, 128, 32, 256, 16, 3]\n",
    "RS_CHANNELS = [6, 8, 16, 32, 16, 8, 3, 8, 16, 32, 16, 8, 6]\n",
    "#RS_CHANNELS = [6, 32, 128, 256, 128, 32, 256, 16, 6]\n",
    "#RS_CHANNELS = [6, 32, 128, 256, 64, 32, 6]\n",
    "CHANNELS     = {0:SET_CHANNELS, 1:GRAPH_CHANNELS, 2:None, 3:RS_CHANNELS}\n",
    "NBODY_MODELS = {0:models.SetModel, 1:models.GraphModel, 2:models.VelocityScaled, 3:models.RSModel}\n",
    "MTAGS        = {0:'S', 1:'G', 2:'V', 3:'R'}\n",
    "RS_IDX = {6.0:0, 4.0:1, 2.0:2, 1.5:3, 1.2:4, 1.0:5, 0.8:6, 0.6:7, 0.4:8, 0.2:9, 0.0:10} # for new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Training and model params\n",
    "#=============================================================================\n",
    "use_gpu = True\n",
    "xp = cupy if use_gpu == 1 else np\n",
    "num_iters = 500\n",
    "num_particles = 32\n",
    "zX, zY = 6.0, 0.0\n",
    "mb_size = 2\n",
    "model_type = 3 # {0:set, 1:graph, 2:vel}\n",
    "mtype  = NBODY_MODELS[model_type]\n",
    "channels = CHANNELS[model_type]\n",
    "mname  = 'GRAPH_HAT_TEST_'\n",
    "theta = None\n",
    "use_theta = 0\n",
    "save_label = data_utils.get_save_label(mname, MTAGS[model_type], use_theta, num_particles, zX, zY)\n",
    "if use_theta == 1:\n",
    "    theta = np.load('./thetas_timesteps.npy').item()\n",
    "    #theta_val = thetas[(num_particles, zX, zY)]['W']\n",
    "    #theta = theta_val\n",
    "\n",
    "model_dir = './testing/'\n",
    "loss_path = model_dir + 'Loss/'\n",
    "if not os.path.exists(loss_path): os.makedirs(loss_path)\n",
    "    \n",
    "def seed_rng(s=12345):\n",
    "    np.random.seed(s)\n",
    "    xp.random.seed(s)\n",
    "seed_rng()\n",
    "print('Training {}, save: {}'.format(save_label, model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_validation(X, num_val_samples=200):\n",
    "    \"\"\" split dataset into training and validation sets\n",
    "    \n",
    "    Args:        \n",
    "        X, Y (ndarray): data arrays of shape (num_samples, num_particles, 6)\n",
    "        num_val_samples (int): size of validation set\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[1]\n",
    "    idx_list = np.random.permutation(num_samples)\n",
    "    X = X[:,idx_list]\n",
    "    X_train, X_val = X[:,:-num_val_samples], X[:,-num_val_samples:]\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Load data\n",
    "#=============================================================================\n",
    "X = np.load('/home/evan/Data/nbody_simulations/ALL_{}.npy'.format(num_particles)) # 'ALL_32'\n",
    "X = data_utils.normalize(X)\n",
    "#if use_gpu: X = cuda.to_gpu(X)\n",
    "X_train, X_val = split_data_validation(X)\n",
    "X = None # need to reduce memory overhead for larger dataset\n",
    "print('X_train.shape: {}'.format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PROCESS USED TO CREATE FULL RS DATA\n",
    "# DATA INTEGRITY WAS TESTED\n",
    "DATA = data_utils.load_data(32, 6.0, 4.0, 2.0, 1.5, 1.2, 1.0, 0.8, 0.6, 0.4, 0.2, 0.0, normalize_data=True)\n",
    "\n",
    "DATA[0].shape\n",
    "\n",
    "data_arr = np.zeros(((len(DATA),) + DATA[0].shape)).astype(np.float32)\n",
    "\n",
    "for i in range(len(DATA)):\n",
    "    data_arr[i] = DATA[i]\n",
    "\n",
    "np.save('/home/evan/Data/nbody_simulations/ALL_32', data_arr)\n",
    "data_arr.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Loss history\n",
    "#=============================================================================\n",
    "train_loss_history = np.zeros((num_iters)).astype(np.float32) # epoch loss hist\n",
    "validation_loss_history = np.zeros((X_val.shape[1])).astype(np.float32) # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Init model and opt\n",
    "#=============================================================================\n",
    "model_save_label = save_label + '{}_'.format(RNG_SEED)\n",
    "print('model tag: {}'.format(model_save_label))\n",
    "seed_rng(RNG_SEED)\n",
    "model = mtype(channels, layer=models.GraphModel,theta=theta)\n",
    "if use_gpu: model.to_gpu()\n",
    "optimizer = optimizers.Adam(alpha=LEARNING_RATE)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch based train loop\n",
    "NOT WORKING\n",
    "'''\n",
    "\"\"\"\n",
    "#denom = 2*(2**(1/2.)) # 2 * sqrt(2)\n",
    "#eps = 0.095\n",
    "#bound = ((-1/denom) + eps, (1/denom) - eps)\n",
    "fwd_type = 0 # [0: fwd_external, 1: fwd_diffs, 2: fwd_preds]\n",
    "rs_start = RS_IDX[zX]\n",
    "rs_target = RS_IDX[zY]\n",
    "bound = (BOUND, 1-BOUND)\n",
    "for cur_epoch in range(num_epochs):\n",
    "    shuffled_idx = xp.random.choice(X_train.shape[1], X_train.shape[1])\n",
    "    X_train = X_train[:,shuffled_idx]\n",
    "    for cur_iter in range(X_train.shape[1]):\n",
    "        model.zerograds() # must always zero grads before another forward pass!    \n",
    "        # create mini-batches for training\n",
    "        _x_in = X_train[:,cur_iter] # shape (11, 1, 4096, 6), retaining dim for convenience downstream\n",
    "        _x_in = random_augmentation_shift(_x_in)[:,None,...] # expand dim lost\n",
    "        x_in = chainer.Variable(_x_in)\n",
    "        if model_type == 3:\n",
    "            # fwd\n",
    "            if fwd_type == 0: # external\n",
    "                x_hat, loss = model.fwd_input(x_in)\n",
    "            elif fwd_type == 1:\n",
    "                x_hat, bounded_squared_error = model.fwd_input_pred_diff(x_in)\n",
    "                loss = F.mean(F.sum(bounded_squared_error, axis=-1))\n",
    "            else:\n",
    "                #rs_start, rs_target = fwd_type\n",
    "                x_hat = model.fwd_pred(x_in, rs_tup=(rs_start, rs_target))\n",
    "                loss = nn.get_bounded_MSE(x_hat, x_in[rs_target], bound)\n",
    "        elif model_type == 4:\n",
    "            x_hat, loss = model.fwd_input_pred_diff(x_in)\n",
    "            #loss = nn.get_bounded_MSE(x_hat, x_in[rs_target], bound)\n",
    "            loss = F.mean(F.sum(F.squared_difference(x_hat[...,:3], x_in[rs_target, :, :, :3])))\n",
    "        else:\n",
    "            x_hat = model(x_in[rs_start], add=True)\n",
    "            loss = nn.get_bounded_MSE(x_hat, x_in[rs_target], bound)\n",
    "\n",
    "        # backprop and update\n",
    "        loss.backward() # this calculates all the gradients (backprop)\n",
    "        optimizer.update() # this updates the weights    \n",
    "        train_loss_history[cur_epoch, cur_iter] = cuda.to_cpu(loss.data)\n",
    "        if cur_iter % 10 == 0 and cur_iter != 0:\n",
    "            y = train_loss_history[cur_epoch, :cur_iter]\n",
    "            plt.close('all')\n",
    "            title = '{}:{}, loss: {:.6}'.format(cur_epoch, cur_iter, y[-1])\n",
    "            fig = data_utils.plot_training_curve(y, cur_iter, yclip=-1, title=title)\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(0.0001)\n",
    "    np.save(loss_path + save_label + 'train_loss', train_loss_history)\n",
    "    #print('{}: converged at {}'.format(model_save_label, np.median(lh_train[-150:])))\n",
    "    # save model, optimizer\n",
    "np.save(loss_path + save_label + 'train_loss', train_loss_history)\n",
    "data_utils.save_model([model, optimizer], model_dir + model_save_label)\n",
    "\"\"\"\n",
    "dont_print_cell = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_random_augmentation_shift(batch):\n",
    "    xp = chainer.cuda.get_array_module(batch)\n",
    "    batch_size = batch.shape[1]\n",
    "    rands = xp.random.rand(6)\n",
    "    shift = xp.random.rand(1,batch_size,1,3)\n",
    "    # shape (11, bs, n_P, 6)\n",
    "    if rands[0] < .5:\n",
    "        batch = batch[...,[1,0,2,4,3,5]]\n",
    "        #batch = batch[:,:,:,[1,0,2,4,3,5]]\n",
    "    if rands[1] < .5:\n",
    "        batch = batch[...,[0,2,1,3,5,4]]\n",
    "        #batch = batch[:,:,:, [0,2,1,3,5,4]]\n",
    "    if rands[2] < .5:\n",
    "        batch = batch[...,[2,1,0,5,4,3]]\n",
    "        #batch = batch[:,:,:, [2,1,0,5,4,3]]\n",
    "    if rands[3] < .5:\n",
    "        #batch[:,:,:,0] = 1 - batch[:,:,:,0]\n",
    "        #batch[:,:,:,3] = -batch[:,:,:,3]\n",
    "        batch[...,0] = 1 - batch[...,0]\n",
    "        batch[...,3] = -batch[...,3]\n",
    "    if rands[4] < .5:\n",
    "        #batch[:,:,:,1] = 1 - batch[:,:,:,1]\n",
    "        #batch[:,:,:,4] = -batch[:,:,:,4]\n",
    "        batch[...,1] = 1 - batch[...,1]\n",
    "        batch[...,4] = -batch[...,4]\n",
    "    if rands[5] < .5:\n",
    "        #batch[:,:,:,2] = 1 - batch[:,:,:,2]\n",
    "        #batch[:,:,:,5] = -batch[:,:,:,5]\n",
    "        batch[...,2] = 1 - batch[...,2]\n",
    "        batch[...,5] = -batch[...,5]\n",
    "    batch_coo = batch[...,:3]\n",
    "    batch_coo += shift\n",
    "    gt1 = batch_coo > 1\n",
    "    batch_coo[gt1] = batch_coo[gt1] - 1\n",
    "    batch[...,:3] = batch_coo\n",
    "    return batch\n",
    "\n",
    "def next_multi_minibatch(X_in, batch_size, data_aug='shift'):\n",
    "    index_list = np.random.choice(X_in.shape[1], batch_size)\n",
    "    batches = X_in[:,index_list]\n",
    "    if data_aug == 'shift':\n",
    "        return multi_random_augmentation_shift(batches)\n",
    "    else:\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iteration based train loop\n",
    "'''\n",
    "#denom = 2*(2**(1/2.)) # 2 * sqrt(2)\n",
    "#eps = 0.095\n",
    "#bound = ((-1/denom) + eps, (1/denom) - eps)S\n",
    "#fwd_type = 0 # [0: fwd_external, 1: fwd_diffs, 2: fwd_preds]\n",
    "rs_start  = RS_IDX[zX]\n",
    "rs_target = RS_IDX[zY]\n",
    "bound = (BOUND, 1-BOUND)\n",
    "seed_rng()\n",
    "thresholds = np.linspace(0.95, 0.5, num_iters)\n",
    "thresholds[num_iters//2:] = 0.5\n",
    "mb_size = 4\n",
    "\n",
    "#if use_gpu: X_train = cuda.to_gpu(X_train)\n",
    "for cur_iter in range(num_iters):\n",
    "    model.zerograds() # must always zero grads before another forward pass!\n",
    "    # get prediction and loss\n",
    "    if model_type == 3:\n",
    "        _x_in = next_multi_minibatch(X_train, mb_size)\n",
    "        if use_gpu: _x_in = cuda.to_gpu(_x_in)\n",
    "        x_in = chainer.Variable(_x_in)\n",
    "        x_hat, loss = model.fwd_pred_loss(x_in)\n",
    "        #x_hat, loss = model.fwd_input_sched(x_in, thresholds[cur_iter])\n",
    "    else:\n",
    "        # create mini-batches for training\n",
    "        _x_in, _x_true = data_utils.next_minibatch([X_train[rs_start], X_train[rs_target]], mb_size, data_aug='shift')\n",
    "        x_in, x_true = chainer.Variable(_x_in), chainer.Variable(_x_true)\n",
    "        x_hat = model(x_in, add=True) # prediction\n",
    "        loss = nn.get_bounded_MSE(x_hat, x_true, boundary=bound) # bound = 0.095\n",
    "    \n",
    "    # backprop and update\n",
    "    loss.backward() # this calculates all the gradients (backprop)\n",
    "    optimizer.update() # this updates the weights\n",
    "    \n",
    "    train_loss_history[cur_iter] = cuda.to_cpu(loss.data)\n",
    "    if cur_iter % 10 == 0 and cur_iter != 0:\n",
    "        y = train_loss_history[:cur_iter]\n",
    "        plt.close('all')\n",
    "        fig = data_utils.plot_training_curve(y, cur_iter, yclip=-1)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.0001)\n",
    "\n",
    "np.save(loss_path + save_label + 'train_loss', train_loss_history)\n",
    "#print('{}: converged at {}'.format(model_save_label, np.median(lh_train[-150:])))\n",
    "# save model, optimizer\n",
    "data_utils.save_model([model, optimizer], model_dir + model_save_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7689549824\n",
    "16777216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}: converged at {}'.format(model_save_label, np.median(train_loss_history[-150:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = ['SL_16_6000_train_loss.npy',\n",
    "               'SETRL_16_6000_train_loss.npy',\n",
    "               #'SET_SCHEDRL_16_6000_train_loss.npy',\n",
    "               'SET_HATRL_16_6000_train_loss.npy',\n",
    "               #'SET_HATR_32_6000_train_loss.npy',\n",
    "              ]\n",
    "lh_dict = {llabel:np.load(loss_path + llabel) for llabel in loss_labels}\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.grid()\n",
    "pstart = 200\n",
    "for label, loss in lh_dict.items():\n",
    "    plt.plot(loss[pstart:], label=label)\n",
    "plt.legend()\n",
    "display.display(plt.gcf())\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.load('thetas_timesteps.npy').item()\n",
    "theta = thetas[(num_particles, zX, zY)]['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "#validation_loss_history = np.zeros((num_val_batches))\n",
    "rs_start = RS_IDX[zX]\n",
    "rs_target = RS_IDX[zY]\n",
    "val_rs_tuple_pred = (rs_start, rs_target) # 0.6 -> 0.0\n",
    "predictions = xp.zeros(((rs_target - rs_start,) + X_val.shape[1:])).astype(xp.float32)\n",
    "with chainer.using_config('train', False):\n",
    "    for val_iter in range(X_val.shape[1]):\n",
    "        _val_in   = X_val[rs_start,  val_iter:val_iter+1] # shape (1,4096,6)\n",
    "        _val_true = X_val[rs_target, val_iter:val_iter+1] # shape (1,4096,6)\n",
    "        if use_gpu:\n",
    "            _val_in = cuda.to_gpu(_val_in)\n",
    "            _val_true = cuda.to_gpu(_val_true)\n",
    "        val_in, val_true = chainer.Variable(_val_in), chainer.Variable(_val_true)\n",
    "        if model_type == 3: \n",
    "            val_hat, preds  = model.fwd_pred(val_in, val_rs_tuple_pred)\n",
    "            predictions[:,val_iter] = preds[:,0]            \n",
    "        else: val_hat = model(val_in)\n",
    "        #val_hat = val_in.data[...,:3] + theta*val_in.data[...,3:]\n",
    "        #val_loss = nn.mean_squared_error(val_in, val_true)\n",
    "        val_loss = nn.mean_squared_error(val_hat, val_true)\n",
    "        \n",
    "        validation_loss_history[val_iter] = cuda.to_cpu(val_loss.data)\n",
    "        print('{:<3}: {:.8}'.format(val_iter, validation_loss_history[val_iter]))\n",
    "    #np.save(loss_path + save_label + 'val_loss', validation_loss_history)\n",
    "    print('{}: validation avg {}'.format(model_save_label, np.mean(validation_loss_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = ['SL_16_6000_val_loss.npy',\n",
    "               #'SETRL_16_6000_val_loss.npy',\n",
    "               #'SET_SCHEDRL_16_6000_val_loss.npy',\n",
    "               'SET_HATRL_16_6000_val_loss.npy',\n",
    "               #'SET_HATR_32_6000_val_loss.npy',\n",
    "              ]\n",
    "lh_dict = {llabel:np.load(loss_path + llabel) for llabel in loss_labels}\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.grid()\n",
    "for label, loss in lh_dict.items():\n",
    "    avg = np.mean(loss)\n",
    "    label = '{}: {}'.format(label[:15], avg)\n",
    "    plt.plot(loss, label=label)\n",
    "#plt.plot(validation_loss_history)\n",
    "plt.legend()\n",
    "display.display(plt.gcf())\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rot2lh = validation_loss_history\n",
    "xp.mean(xp.sum(xp.square(X_val[...,:3] - Y_val[...,:3]), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "0  : 0.00071449857\n",
    "1  : 0.00083440135\n",
    "2  : 0.00069077744\n",
    "3  : 0.00076021155\n",
    "4  : 0.00066414644\n",
    "5  : 0.00076436519\n",
    "6  : 0.00068148802\n",
    "7  : 0.00077487406\n",
    "8  : 0.00075997895\n",
    "9  : 0.00072597218\n",
    "10 : 0.00074742711\n",
    "11 : 0.0006032426\n",
    "12 : 0.0007503003\n",
    "13 : 0.0007320182\n",
    "14 : 0.00079973473\n",
    "15 : 0.00073265145\n",
    "16 : 0.00074171007\n",
    "17 : 0.00076671556\n",
    "18 : 0.00073367765\n",
    "19 : 0.00074930477\n",
    "20 : 0.00067527685\n",
    "21 : 0.00074823305\n",
    "22 : 0.00077885698\n",
    "23 : 0.00079977652\n",
    "24 : 0.0007059208\n",
    "Rot2GL_16_0400_77743196_: validation avg 0.0007374224159866572\n",
    "    \n",
    "0  : 0.00055363186\n",
    "1  : 0.0006731872\n",
    "2  : 0.00058509264\n",
    "3  : 0.0006904325\n",
    "4  : 0.00056985248\n",
    "5  : 0.00058646151\n",
    "6  : 0.00057804317\n",
    "7  : 0.00069477817\n",
    "8  : 0.00058836123\n",
    "9  : 0.00064376084\n",
    "10 : 0.00058327831\n",
    "11 : 0.00052094873\n",
    "12 : 0.00069097709\n",
    "13 : 0.00060718646\n",
    "14 : 0.00060357916\n",
    "15 : 0.00063511333\n",
    "16 : 0.00061753101\n",
    "17 : 0.00061934261\n",
    "18 : 0.00070599839\n",
    "19 : 0.00063490745\n",
    "20 : 0.00057715503\n",
    "21 : 0.00058344653\n",
    "22 : 0.00068214984\n",
    "23 : 0.00060094299\n",
    "24 : 0.00063226151\n",
    "Rot2GL_16_0400_77743196_: validation avg 0.0006183368014171719\n",
    "0  : 0.00063232199\n",
    "1  : 0.00069677189\n",
    "2  : 0.00065076264\n",
    "3  : 0.00073845498\n",
    "4  : 0.00058847119\n",
    "5  : 0.0006447584\n",
    "6  : 0.00062783115\n",
    "7  : 0.00073686329\n",
    "8  : 0.0006396636\n",
    "9  : 0.00069760205\n",
    "10 : 0.0006765736\n",
    "11 : 0.00054225337\n",
    "12 : 0.00072450895\n",
    "13 : 0.00066313695\n",
    "14 : 0.00069704035\n",
    "15 : 0.0006880436\n",
    "16 : 0.00071569113\n",
    "17 : 0.00069740042\n",
    "18 : 0.00073569786\n",
    "19 : 0.00069740979\n",
    "20 : 0.00064883736\n",
    "21 : 0.000624654\n",
    "22 : 0.00073985819\n",
    "23 : 0.00062950642\n",
    "24 : 0.00066471507\n",
    "Rot3G_16_0400_77743196_: validation avg 0.0006719531305134297\n",
    "    \n",
    "0  : 9.3290488e-05\n",
    "1  : 9.1763468e-05\n",
    "2  : 8.4180276e-05\n",
    "3  : 0.00013588619\n",
    "4  : 8.1585691e-05\n",
    "5  : 8.4469924e-05\n",
    "6  : 8.4433683e-05\n",
    "7  : 9.1588248e-05\n",
    "8  : 6.9105059e-05\n",
    "9  : 8.0118167e-05\n",
    "10 : 8.2520099e-05\n",
    "11 : 7.4855241e-05\n",
    "12 : 0.00010116811\n",
    "13 : 8.4584281e-05\n",
    "14 : 0.00010005159\n",
    "15 : 9.2386865e-05\n",
    "16 : 9.5140014e-05\n",
    "17 : 9.0737449e-05\n",
    "18 : 0.00010576588\n",
    "19 : 9.0127731e-05\n",
    "20 : 8.2126113e-05\n",
    "21 : 8.2276296e-05\n",
    "22 : 9.8431243e-05\n",
    "23 : 9.0143083e-05\n",
    "24 : 8.8505287e-05\n",
    "G_16_0400_77743196_: validation avg 9.020961908390746e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot point cloud\n",
    "'''\n",
    "from IPython import display\n",
    "j = 3\n",
    "fsize = (28,28)\n",
    "particle_size = (.6, 1.3)\n",
    "truth_color = 'blue'\n",
    "pred_color = 'red'\n",
    "colors = (truth_color, pred_color)\n",
    "\n",
    "plt.close('all')\n",
    "fig = data_utils.plot_3D_pointcloud(xt, xh, j, colors=colors, pt_size=particle_size, fsize=fsize)\n",
    "plt.gca().view_init(20, 120)\n",
    "display.display(plt.gcf())\n",
    "display.clear_output(wait=True)\n",
    "time.sleep(0.0001)\n",
    "rotate = False\n",
    "if rotate:\n",
    "    for angle in range(0,360,40):\n",
    "        fig.view_init(30, angle)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(0.0001)\n",
    "#fig.savefig('./Plots/' + model_name, dpi=2400,bbox_inches='tight') # warning, this makes a huge image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_all_path = '/home/evan/Data/nbody_simulations/ALL_32.npy'\n",
    "X = np.load(data_all_path)\n",
    "Y = data_utils.normalize(np.copy(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

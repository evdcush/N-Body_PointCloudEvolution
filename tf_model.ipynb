{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, code, sys, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import cuda\n",
    "from random import randrange\n",
    "from numpy import linalg as LA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # IMPLEMENTATION NOTES\\n X network params: need to make it so param Variables are initialized with a fixed label,\\n   so that only one set of network hyper-params are defined in graph at a time\\n   (so tf.Variable(rand_init_values, name=LABEL.format(i))) # this was fixed with tf.variable_scope(*, reuse=tf.AUTO_REUSE)\\n X make fwd and network functions \\n     # done, but not getting loss it should be getting, maybe there just isn't anyway to use TF without hardcoding graph with placeholders and tensors?\\n - been blackboxing Daniele's periodic boundary code, need to confirm that no extra space or extra loops being used\\n - see how framework-agnostic utils functions are. chainer.cuda.get_array_module should not be a problem if numpy data is sent\\n - compare sparse ops for density graph\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # IMPLEMENTATION NOTES\n",
    " X network params: need to make it so param Variables are initialized with a fixed label,\n",
    "   so that only one set of network hyper-params are defined in graph at a time\n",
    "   (so tf.Variable(rand_init_values, name=LABEL.format(i))) # this was fixed with tf.variable_scope(*, reuse=tf.AUTO_REUSE)\n",
    " X make fwd and network functions \n",
    "     # done, but not getting loss it should be getting, maybe there just isn't anyway to use TF without hardcoding graph with placeholders and tensors?\n",
    " - been blackboxing Daniele's periodic boundary code, need to confirm that no extra space or extra loops being used\n",
    " - see how framework-agnostic utils functions are. chainer.cuda.get_array_module should not be a problem if numpy data is sent\n",
    " - compare sparse ops for density graph\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Session, network settings\n",
    "'''\n",
    "params_seed = 98765\n",
    "data_seed   = 12345\n",
    "def seed_rng(s=data_seed):\n",
    "    np.random.seed(s)\n",
    "    tf.set_random_seed(s)\n",
    "    print('seeded by {}'.format(s))\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded by 12345\n",
      "Using redshifts z0.6, z0.0, with 4096 particles\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataset parameters\n",
    "'''\n",
    "num_particles = 16 # defaults 16**3\n",
    "zX = 0.6\n",
    "zY = 0.0\n",
    "#X_input, X_truth = utils.load_data(num_particles, zX, zY, normalize_data=True)\n",
    "rs_start = utils.REDSHIFTS.index(zX)\n",
    "rs_target = utils.REDSHIFTS.index(zY)\n",
    "X = utils.load_npy_data(num_particles) # (11, N, D, 6)\n",
    "X = X[[rs_start, rs_target]] # (2, N, D, 6)\n",
    "X = utils.normalize_fullrs(X)\n",
    "seed_rng()\n",
    "X_train, X_val = utils.multi_split_data_validation(X, num_val_samples=200)\n",
    "X = None # reduce memory overhead\n",
    "#X_input = np.load('X16_06.npy')\n",
    "#X_truth = np.load('X16_00.npy')\n",
    "print('Using redshifts z{}, z{}, with {} particles'.format(zX,zY,num_particles**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_H_LABEL     = 'Wh_{}'\n",
    "WEIGHT_GRAPH_LABEL = 'Wg_{}'\n",
    "BIAS_LABEL   = 'B_{}' # eg 'B_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model parameter initializations\n",
    "'''\n",
    "\n",
    "''' # earlier attempts, without scope, to be deleted\n",
    "def _init_weight(k_in, k_out, var_name):\n",
    "    \"\"\" Initialize weights for fully connected layer\n",
    "    weight drawn from he-normal distribution\n",
    "    Args:\n",
    "        k_in  (int): input channels\n",
    "        k_out (int): output channels\n",
    "    Returns: tf.Variable holding weight of shape (k_in, k_out)\n",
    "    \"\"\"\n",
    "    henorm_std = np.sqrt(2.0 / k_in)\n",
    "    weight = tf.random_normal((k_in, k_out), stddev=henorm_std)\n",
    "    return tf.Variable(weight, name=var_name)\n",
    "\n",
    "def _init_set_weights(k_in, k_out, layer_idx):\n",
    "    \"\"\" initializes weight for fully-connected layer\n",
    "    \"\"\"\n",
    "    Wh = init_weight(k_in, k_out, WEIGHT_H_LABEL.format(layer_idx))\n",
    "    return Wh    \n",
    "\n",
    "def _init_graph_weights(k_in, k_out, layer_idx):\n",
    "    \"\"\" initialize weights for graph layer\n",
    "    Two weights:\n",
    "        Wh : weight for external/hidden input (k_in, k_out)\n",
    "        Wg : weight for graph input (k_in, k_out)\n",
    "    \"\"\"\n",
    "    Wh = init_weight(k_in, k_out, WEIGHT_H_LABEL.format(layer_idx))\n",
    "    Wg = init_weight(k_in, k_out, WEIGHT_GRAPH_LABEL.format(layer_idx))\n",
    "    return Wh, Wg\n",
    "\n",
    "def _init_bias(k_in, k_out, layer_idx):\n",
    "    \"\"\" initalize bias param\n",
    "    Bias initialized to be near zero\n",
    "    Returns: tf.Variable of shape (k_out,) for bias\n",
    "    \"\"\"\n",
    "    bias = np.ones(k_out).astype(np.float32) * 1e-6\n",
    "    return tf.Variable(bias, BIAS_LABEL.format(layer_idx))\n",
    "\n",
    "def _init_params(channels, graph_weights=True, use_bias=False):\n",
    "    \"\"\" initializes all network hyperparameters\n",
    "    Creates a dict with weights and biases associated with each\n",
    "    hidden layer\n",
    "    Args:\n",
    "        channels (list): list of channel sizes\n",
    "        graph_weights: if true, initializes weights for graph model\n",
    "        use_bias: if true, bias params initialized, else None\n",
    "    Returns: params dict containing weight and biases\n",
    "    \"\"\"\n",
    "    weight_init_fun = init_graph_weights if graph_weights else init_set_weights\n",
    "    kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]    \n",
    "    weights = []\n",
    "    biases  = [] if use_bias else None\n",
    "    for idx, ktup in enumerate(kdims):\n",
    "        weights.append(weight_init_fun(*ktup, idx))     \n",
    "        if use_bias: biases.append(init_bias(*ktup, idx))\n",
    "    params = {'Weights': weights, 'Biases': biases}\n",
    "    return params\n",
    "'''\n",
    "dont_print_cell = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using tf scope, get\n",
    "NOT SURE WHY THIS DOESNT WORK, BUT IT NO LIKE\n",
    "- looked into tf.Estimator, thinking it would provide what we need, but there's no way to use it out of the box.\n",
    "   - override model_fn and input_fn to be expected, but need to mess with Dataset and Iterator classes too I think   \n",
    "'''\n",
    "def init_weight(k_in, k_out, var_name):\n",
    "    \"\"\" Initialize weights for fully connected layer\n",
    "    weight drawn from glorot normal distribution\n",
    "    Args:\n",
    "        k_in  (int): input channels\n",
    "        k_out (int): output channels\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Params\", reuse=tf.AUTO_REUSE):\n",
    "        tf.get_variable(var_name, shape=(k_in, k_out), initializer=tf.glorot_normal_initializer())\n",
    "\n",
    "def init_bias(k_in, k_out, layer_idx):\n",
    "    \"\"\" initalize bias param\n",
    "    Bias initialized to be near zero# actually zero for now\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Params\", reuse=tf.AUTO_REUSE):\n",
    "        # should be init with values near 0\n",
    "        tf.get_variable(var_name, shape=(k_out,), initializer=tf.zeros_initializer())\n",
    "\n",
    "def init_gmodel_params(channels, use_bias=False):\n",
    "    kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]\n",
    "    for idx, ktup in enumerate(kdims):\n",
    "        # init external/hidden weights\n",
    "        wh_name = WEIGHT_H_LABEL.format(idx)\n",
    "        init_weight(*ktup, wh_name)\n",
    "        # init graph weights\n",
    "        wg_name = WEIGHT_GRAPH_LABEL.format(idx)        \n",
    "        init_weight(*ktup, wg_name)\n",
    "        # bias\n",
    "        if use_bias:\n",
    "            b_name = BIAS_LABEL.format(idx)\n",
    "            init_bias(*ktup, b_name)\n",
    "\n",
    "def init_model_params(channels, use_bias=False):\n",
    "    kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]\n",
    "    with tf.variable_scope(\"Model\", reuse=tf.AUTO_REUSE):\n",
    "        for idx, ktup in enumerate(kdims):\n",
    "            # init external/hidden weights\n",
    "            wh_name = WEIGHT_H_LABEL.format(idx)\n",
    "            init_weight(*ktup, wh_name)\n",
    "            # bias\n",
    "            if use_bias:\n",
    "                b_name = BIAS_LABEL.format(idx)\n",
    "                init_bias(*ktup, b_name)\n",
    "\n",
    "'''\n",
    "nn ops\n",
    "'''\n",
    "def left_mult(h, w):\n",
    "    return tf.einsum('ijl,lq->ijq', h, w)\n",
    "\n",
    "def linear_fwd(h_in, layer_idx, use_bias):\n",
    "    with tf.variable_scope(\"Params\", reuse=tf.AUTO_REUSE):\n",
    "        weight = tf.get_variable(WEIGHT_H_LABEL.format(layer_idx))\n",
    "        mean   = tf.reduce_mean(h_in, axis=-1, keepdims=True)\n",
    "        h = h_in - mean\n",
    "        h_out = left_mult(h, weight)\n",
    "        if use_bias:\n",
    "            bias = tf.get_variable(BIAS_LABEL.format(layer_idx))\n",
    "            h_out += bias\n",
    "    return h_out\n",
    "\n",
    "def model_fwd(x_in, num_layers, activation=tf.nn.relu, use_bias=False):\n",
    "    with tf.variable_scope(\"Model\", reuse=tf.AUTO_REUSE):\n",
    "        h = x_in\n",
    "        for i in range(num_layers):\n",
    "            #print('model_fwd: {}'.format(i))\n",
    "            h = linear_fwd(h, i, use_bias)\n",
    "            if i != num_layers - 1:\n",
    "                h = activation(h)\n",
    "                print('relu')\n",
    "    return h\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "def get_readout(h_out): # confirmed same as chainer\n",
    "    gt_one  = (tf.sign(h_out - 1) + 1) / 2\n",
    "    ls_zero = -(tf.sign(h_out) - 1) / 2\n",
    "    rest = 1 - gt_one - ls_zero\n",
    "    readout = rest*h_out + gt_one*(h_out - 1) + ls_zero*(1 + h_out)\n",
    "    return readout\n",
    "\n",
    "def periodic_boundary_dist(readout, x_truth): # confirmed same as chainer\n",
    "    x_truth_coo = x_truth[...,:3]\n",
    "    dist = tf.minimum(tf.square(readout - x_truth_coo), tf.square(readout - (1 + x_truth_coo)))\n",
    "    dist = tf.minimum(dist, tf.square((1 + readout) - x_truth_coo))\n",
    "    return dist\n",
    "    \n",
    "def pbc_loss(readout, x_truth):\n",
    "    \"\"\" has been confirmed to be same as chainer implementation\n",
    "    \"\"\"\n",
    "    pbc_dist = periodic_boundary_dist(readout, x_truth)\n",
    "    pbc_error = tf.reduce_mean(tf.reduce_sum(pbc_dist, axis=-1))\n",
    "    return pbc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "simpler stuff for hardcoded network\n",
    "duplicate code from other cells\n",
    "'''\n",
    "def init_weight(k_in, k_out, name, scale=1.0):\n",
    "    std = scale * np.sqrt(2. / k_in)\n",
    "    henorm = tf.random_normal((k_in, k_out), stddev=std)\n",
    "    W = tf.Variable(henorm, name=name, dtype=tf.float32)\n",
    "    return W\n",
    "\n",
    "def init_bias(k_in, k_out, name):\n",
    "    b_val = np.ones((k_out,)) * 1e-6\n",
    "    b = tf.Variable(b_val, name=name, dtype=tf.float32)\n",
    "    return b    \n",
    "\n",
    "def left_mult(h, W):\n",
    "    return tf.einsum('ijl,lq->ijq', h, W)\n",
    "\n",
    "def linear_layer(h, W, b):\n",
    "    \"\"\" permutation equivariant linear transformation\n",
    "    Args:\n",
    "        h: external input, of shape (mb_size, n_P, k_in)\n",
    "        W: layer weight, of shape (k_in, k_out)\n",
    "        b: bias, of shape (k_out,)\n",
    "    \"\"\"\n",
    "    mu = tf.reduce_mean(h, axis=1, keepdims=True)\n",
    "    h_out = h - mu\n",
    "    h_out = left_mult(h, W) + b\n",
    "    return h_out\n",
    "\n",
    "def get_readout(h_out):\n",
    "    gt_one  = (tf.sign(h_out - 1) + 1) / 2\n",
    "    ls_zero = -(tf.sign(h_out) - 1) / 2\n",
    "    rest = 1 - gt_one - ls_zero\n",
    "    readout = rest*h_out + gt_one*(h_out - 1) + ls_zero*(1 + h_out)\n",
    "    return readout\n",
    "\n",
    "def periodic_boundary_dist(readout, x_truth):\n",
    "    x_truth_coo = x_truth[...,:3]\n",
    "    dist = tf.minimum(tf.square(readout - x_truth_coo), tf.square(readout - (1 + x_truth_coo)))\n",
    "    dist = tf.minimum(dist, tf.square((1 + readout) - x_truth_coo))\n",
    "    return dist\n",
    "    \n",
    "def pbc_loss(readout, x_truth):\n",
    "    pbc_dist = periodic_boundary_dist(readout, x_truth)\n",
    "    pbc_error = tf.reduce_mean(tf.reduce_sum(pbc_dist, axis=-1), name='loss')\n",
    "    return pbc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "disgusting hardcoded tensorflow boilerplate placeholder/tensor bullshit\n",
    "network\n",
    "'''\n",
    "# var naming\n",
    "WEIGHT_TAG = 'W_{}'\n",
    "BIAS_TAG   = 'B_{}' # eg 'B_6'\n",
    "\n",
    "# network params\n",
    "channels = [6, 8, 16, 32, 16, 8, 3, 8, 16, 32, 16, 8, 3]\n",
    "kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]\n",
    "lr = 0.01\n",
    "activation = tf.nn.relu\n",
    "\n",
    "# external data\n",
    "X_input = tf.placeholder(tf.float32, shape=[None, num_particles**3, 6], name='X_input')\n",
    "X_truth = tf.placeholder(tf.float32, shape=[None, num_particles**3, 6], name='X_truth')\n",
    "\n",
    "# network layers and hyper params, 12 layers\n",
    "#============================================================================= H0\n",
    "W0 = init_weight(*kdims[0], WEIGHT_TAG.format(0))\n",
    "B0 = init_bias(  *kdims[0],   BIAS_TAG.format(0))\n",
    "H0 = activation(linear_layer(X_input, W0, B0))\n",
    "\n",
    "#============================================================================= H1\n",
    "W1 = init_weight(*kdims[1], WEIGHT_TAG.format(1))\n",
    "B1 = init_bias(  *kdims[1],   BIAS_TAG.format(1))\n",
    "H1 = activation(linear_layer(H0, W1, B1))\n",
    "\n",
    "#============================================================================= H2\n",
    "W2 = init_weight(*kdims[2], WEIGHT_TAG.format(2))\n",
    "B2 = init_bias(  *kdims[2],   BIAS_TAG.format(2))\n",
    "H2 = activation(linear_layer(H1, W2, B2))\n",
    "\n",
    "#============================================================================= H3\n",
    "W3 = init_weight(*kdims[3], WEIGHT_TAG.format(3))\n",
    "B3 = init_bias(  *kdims[3],   BIAS_TAG.format(3))\n",
    "H3 = activation(linear_layer(H2, W3, B3))\n",
    "\n",
    "#============================================================================= H4\n",
    "W4 = init_weight(*kdims[4], WEIGHT_TAG.format(4))\n",
    "B4 = init_bias(  *kdims[4],   BIAS_TAG.format(4))\n",
    "H4 = activation(linear_layer(H3, W4, B4))\n",
    "\n",
    "#============================================================================= H5\n",
    "W5 = init_weight(*kdims[5], WEIGHT_TAG.format(5))\n",
    "B5 = init_bias(  *kdims[5],   BIAS_TAG.format(5))\n",
    "H5 = activation(linear_layer(H4, W5, B5))\n",
    "\n",
    "#============================================================================= H6\n",
    "W6 = init_weight(*kdims[6], WEIGHT_TAG.format(6))\n",
    "B6 = init_bias(  *kdims[6],   BIAS_TAG.format(6))\n",
    "H6 = activation(linear_layer(H5, W6, B6))\n",
    "\n",
    "#============================================================================= H7\n",
    "W7 = init_weight(*kdims[7], WEIGHT_TAG.format(7))\n",
    "B7 = init_bias(  *kdims[7],   BIAS_TAG.format(7))\n",
    "H7 = activation(linear_layer(H6, W7, B7))\n",
    "\n",
    "#============================================================================= H8\n",
    "W8 = init_weight(*kdims[8], WEIGHT_TAG.format(8))\n",
    "B8 = init_bias(  *kdims[8],   BIAS_TAG.format(8))\n",
    "H8 = activation(linear_layer(H7, W8, B8))\n",
    "\n",
    "#============================================================================= H9\n",
    "W9 = init_weight(*kdims[9], WEIGHT_TAG.format(9))\n",
    "B9 = init_bias(  *kdims[9],   BIAS_TAG.format(9))\n",
    "H9 = activation(linear_layer(H8, W9, B9))\n",
    "\n",
    "#============================================================================= H10\n",
    "W10 = init_weight(*kdims[10], WEIGHT_TAG.format(10))\n",
    "B10 = init_bias(  *kdims[10],   BIAS_TAG.format(10))\n",
    "H10 = activation(linear_layer(H9, W10, B10))\n",
    "\n",
    "#============================================================================= H11\n",
    "W11 = init_weight(*kdims[11], WEIGHT_TAG.format(11))\n",
    "B11 = init_bias(  *kdims[11],   BIAS_TAG.format(11))\n",
    "H11 = linear_layer(H10, W11, B11)\n",
    "\n",
    "#============================================================================= output\n",
    "readout = get_readout(H11)\n",
    "loss  = pbc_loss(readout, X_truth)\n",
    "train = tf.train.AdamOptimizer(lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model\n",
    "'''\n",
    "#channels = [6, 8, 16, 32, 16, 8, 3, 8, 16, 32, 16, 8, 3]\n",
    "#num_layers = len(channels) - 1\n",
    "#learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_iters = 5000\n",
    "#seed_rng(params_seed)\n",
    "#init_graph_params(channels)\n",
    "#init_model_params(channels, use_bias=use_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_in   = X_train[0]\n",
    "#X_true = X_train[1]\n",
    "#x_input = tf.constant(X_in[:8])\n",
    "#x_truth = tf.constant(X_true[:8])\n",
    "#x_hat = model_fwd(x_input, num_layers, use_bias=use_bias)\n",
    "#loss = pbc_loss(x_hat, x_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOptimizer\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Optimizer\n",
    "'''\n",
    "#X_input = tf.placeholder(tf.float32, shape=[None,num_particles**3, channels[0]], name='X_input')\n",
    "#X_truth = tf.placeholder(tf.float32, shape=[None,num_particles**3, channels[0]], name='X_truth')\n",
    "#X_hat   = model_fwd(X_input, num_layers, use_bias=use_bias)\n",
    "\n",
    "#loss    = pbc_loss(X_hat, X_truth)\n",
    "#optimizer = tf.train.AdamOptimizer(0.01)\n",
    "#train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.250359\n",
      "10: 0.231360\n",
      "20: 0.199683\n",
      "30: 0.181704\n",
      "40: 0.173975\n",
      "50: 0.170235\n",
      "60: 0.163188\n",
      "70: 0.158156\n",
      "80: 0.150116\n",
      "90: 0.139051\n",
      "100: 0.136832\n",
      "110: 0.135564\n",
      "120: 0.132356\n",
      "130: 0.135941\n",
      "140: 0.129409\n",
      "150: 0.129283\n",
      "160: 0.129178\n",
      "170: 0.126082\n",
      "180: 0.123815\n",
      "190: 0.127834\n",
      "200: 0.120810\n",
      "210: 0.124295\n",
      "220: 0.119961\n",
      "230: 0.144004\n",
      "240: 0.136999\n",
      "250: 0.128954\n",
      "260: 0.123184\n",
      "270: 0.119444\n",
      "280: 0.115952\n",
      "290: 0.131851\n",
      "300: 0.116649\n",
      "310: 0.109736\n",
      "320: 0.103474\n",
      "330: 0.101490\n",
      "340: 0.093387\n",
      "350: 0.094134\n",
      "360: 0.091371\n",
      "370: 0.090302\n",
      "380: 0.085266\n",
      "390: 0.084868\n",
      "400: 0.082238\n",
      "410: 0.081378\n",
      "420: 0.083361\n",
      "430: 0.077598\n",
      "440: 0.073960\n",
      "450: 0.073557\n",
      "460: 0.072364\n",
      "470: 0.074591\n",
      "480: 0.072591\n",
      "490: 0.082436\n",
      "500: 0.073130\n",
      "510: 0.069050\n",
      "520: 0.066847\n",
      "530: 0.065198\n",
      "540: 0.089780\n",
      "550: 0.055885\n",
      "560: 0.047694\n",
      "570: 0.048164\n",
      "580: 0.049698\n",
      "590: 0.041952\n",
      "600: 0.038307\n",
      "610: 0.037599\n",
      "620: 0.035168\n",
      "630: 0.030955\n",
      "640: 0.032225\n",
      "650: 0.028142\n",
      "660: 0.029118\n",
      "670: 0.029700\n",
      "680: 0.039385\n",
      "690: 0.027861\n",
      "700: 0.025781\n",
      "710: 0.025464\n",
      "720: 0.024822\n",
      "730: 0.026252\n",
      "740: 0.023114\n",
      "750: 0.022754\n",
      "760: 0.022984\n",
      "770: 0.023636\n",
      "780: 0.023138\n",
      "790: 0.021369\n",
      "800: 0.023240\n",
      "810: 0.022170\n",
      "820: 0.021527\n",
      "830: 0.021423\n",
      "840: 0.021904\n",
      "850: 0.021046\n",
      "860: 0.029056\n",
      "870: 0.021901\n",
      "880: 0.021017\n",
      "890: 0.019916\n",
      "900: 0.020294\n",
      "910: 0.020524\n",
      "920: 0.019224\n",
      "930: 0.019682\n",
      "940: 0.020034\n",
      "950: 0.019516\n",
      "960: 0.017658\n",
      "970: 0.018301\n",
      "980: 0.025045\n",
      "990: 0.020279\n",
      "1000: 0.020228\n",
      "1010: 0.018060\n",
      "1020: 0.018603\n",
      "1030: 0.018944\n",
      "1040: 0.018505\n",
      "1050: 0.018594\n",
      "1060: 0.019429\n",
      "1070: 0.019939\n",
      "1080: 0.017985\n",
      "1090: 0.018240\n",
      "1100: 0.017810\n",
      "1110: 0.017126\n",
      "1120: 0.018334\n",
      "1130: 0.019178\n",
      "1140: 0.017919\n",
      "1150: 0.017491\n",
      "1160: 0.017617\n",
      "1170: 0.016767\n",
      "1180: 0.018796\n",
      "1190: 0.017730\n",
      "1200: 0.018307\n",
      "1210: 0.015825\n",
      "1220: 0.017020\n",
      "1230: 0.018611\n",
      "1240: 0.018710\n",
      "1250: 0.017107\n",
      "1260: 0.016471\n",
      "1270: 0.015307\n",
      "1280: 0.016939\n",
      "1290: 0.021595\n",
      "1300: 0.018727\n",
      "1310: 0.017131\n",
      "1320: 0.017868\n",
      "1330: 0.016576\n",
      "1340: 0.016388\n",
      "1350: 0.016172\n",
      "1360: 0.015768\n",
      "1370: 0.017231\n",
      "1380: 0.023847\n",
      "1390: 0.016691\n",
      "1400: 0.015999\n",
      "1410: 0.016221\n",
      "1420: 0.015996\n",
      "1430: 0.015535\n",
      "1440: 0.015179\n",
      "1450: 0.015918\n",
      "1460: 0.014373\n",
      "1470: 0.015676\n",
      "1480: 0.014911\n",
      "1490: 0.015815\n",
      "1500: 0.014625\n",
      "1510: 0.014230\n",
      "1520: 0.014493\n",
      "1530: 0.014032\n",
      "1540: 0.014104\n",
      "1550: 0.014825\n",
      "1560: 0.014329\n",
      "1570: 0.017934\n",
      "1580: 0.018501\n",
      "1590: 0.017973\n",
      "1600: 0.017529\n",
      "1610: 0.016264\n",
      "1620: 0.014424\n",
      "1630: 0.013811\n",
      "1640: 0.013592\n",
      "1650: 0.013637\n",
      "1660: 0.014105\n",
      "1670: 0.015519\n",
      "1680: 0.016514\n",
      "1690: 0.015914\n",
      "1700: 0.016379\n",
      "1710: 0.014174\n",
      "1720: 0.014993\n",
      "1730: 0.013845\n",
      "1740: 0.011524\n",
      "1750: 0.011566\n",
      "1760: 0.011603\n",
      "1770: 0.012617\n",
      "1780: 0.012621\n",
      "1790: 0.013117\n",
      "1800: 0.013573\n",
      "1810: 0.012635\n",
      "1820: 0.012806\n",
      "1830: 0.011723\n",
      "1840: 0.015471\n",
      "1850: 0.016460\n",
      "1860: 0.012722\n",
      "1870: 0.010243\n",
      "1880: 0.010450\n",
      "1890: 0.009207\n",
      "1900: 0.010231\n",
      "1910: 0.011700\n",
      "1920: 0.011358\n",
      "1930: 0.011246\n",
      "1940: 0.010891\n",
      "1950: 0.010654\n",
      "1960: 0.021220\n",
      "1970: 0.018181\n",
      "1980: 0.012033\n",
      "1990: 0.010071\n",
      "2000: 0.009738\n",
      "2010: 0.008586\n",
      "2020: 0.008641\n",
      "2030: 0.008497\n",
      "2040: 0.008397\n",
      "2050: 0.008057\n",
      "2060: 0.009174\n",
      "2070: 0.008298\n",
      "2080: 0.008419\n",
      "2090: 0.011854\n",
      "2100: 0.008961\n",
      "2110: 0.009187\n",
      "2120: 0.030273\n",
      "2130: 0.059806\n",
      "2140: 0.030429\n",
      "2150: 0.017967\n",
      "2160: 0.013008\n",
      "2170: 0.009606\n",
      "2180: 0.009278\n",
      "2190: 0.009410\n",
      "2200: 0.009228\n",
      "2210: 0.008723\n",
      "2220: 0.008563\n",
      "2230: 0.008327\n",
      "2240: 0.007975\n",
      "2250: 0.007966\n",
      "2260: 0.007568\n",
      "2270: 0.008766\n",
      "2280: 0.008403\n",
      "2290: 0.008106\n",
      "2300: 0.007374\n",
      "2310: 0.007643\n",
      "2320: 0.007492\n",
      "2330: 0.008308\n",
      "2340: 0.008557\n",
      "2350: 0.007348\n",
      "2360: 0.007205\n",
      "2370: 0.006777\n",
      "2380: 0.006315\n",
      "2390: 0.009851\n",
      "2400: 0.006826\n",
      "2410: 0.008339\n",
      "2420: 0.012999\n",
      "2430: 0.009814\n",
      "2440: 0.008620\n",
      "2450: 0.007263\n",
      "2460: 0.006746\n",
      "2470: 0.006469\n",
      "2480: 0.006453\n",
      "2490: 0.006127\n",
      "2500: 0.007091\n",
      "2510: 0.006199\n",
      "2520: 0.008241\n",
      "2530: 0.041670\n",
      "2540: 0.015368\n",
      "2550: 0.010477\n",
      "2560: 0.009289\n",
      "2570: 0.009189\n",
      "2580: 0.008796\n",
      "2590: 0.008011\n",
      "2600: 0.007336\n",
      "2610: 0.007614\n",
      "2620: 0.008307\n",
      "2630: 0.007335\n",
      "2640: 0.007460\n",
      "2650: 0.007537\n",
      "2660: 0.006129\n",
      "2670: 0.006266\n",
      "2680: 0.007774\n",
      "2690: 0.007634\n",
      "2700: 0.020283\n",
      "2710: 0.007253\n",
      "2720: 0.007182\n",
      "2730: 0.006807\n",
      "2740: 0.008822\n",
      "2750: 0.009609\n",
      "2760: 0.009767\n",
      "2770: 0.008858\n",
      "2780: 0.007369\n",
      "2790: 0.005847\n",
      "2800: 0.006097\n",
      "2810: 0.005862\n",
      "2820: 0.006228\n",
      "2830: 0.005743\n",
      "2840: 0.006275\n",
      "2850: 0.006279\n",
      "2860: 0.006802\n",
      "2870: 0.007829\n",
      "2880: 0.006458\n",
      "2890: 0.013097\n",
      "2900: 0.005343\n",
      "2910: 0.005853\n",
      "2920: 0.007481\n",
      "2930: 0.006350\n",
      "2940: 0.005055\n",
      "2950: 0.011902\n",
      "2960: 0.005441\n",
      "2970: 0.004931\n",
      "2980: 0.003696\n",
      "2990: 0.004373\n",
      "3000: 0.005531\n",
      "3010: 0.009595\n",
      "3020: 0.005198\n",
      "3030: 0.004875\n",
      "3040: 0.005037\n",
      "3050: 0.006156\n",
      "3060: 0.005751\n",
      "3070: 0.004375\n",
      "3080: 0.003958\n",
      "3090: 0.003748\n",
      "3100: 0.004990\n",
      "3110: 0.005044\n",
      "3120: 0.005090\n",
      "3130: 0.005302\n",
      "3140: 0.005430\n",
      "3150: 0.006432\n",
      "3160: 0.005301\n",
      "3170: 0.005091\n",
      "3180: 0.005244\n",
      "3190: 0.005341\n",
      "3200: 0.005541\n",
      "3210: 0.005979\n",
      "3220: 0.005442\n",
      "3230: 0.005697\n",
      "3240: 0.005798\n",
      "3250: 0.006496\n",
      "3260: 0.011847\n",
      "3270: 0.010050\n",
      "3280: 0.050668\n",
      "3290: 0.026930\n",
      "3300: 0.012133\n",
      "3310: 0.008487\n",
      "3320: 0.006094\n",
      "3330: 0.007848\n",
      "3340: 0.011469\n",
      "3350: 0.009132\n",
      "3360: 0.007491\n",
      "3370: 0.006317\n",
      "3380: 0.005896\n",
      "3390: 0.009928\n",
      "3400: 0.007134\n",
      "3410: 0.006544\n",
      "3420: 0.005406\n",
      "3430: 0.006228\n",
      "3440: 0.004246\n",
      "3450: 0.004718\n",
      "3460: 0.004153\n",
      "3470: 0.003094\n",
      "3480: 0.003585\n",
      "3490: 0.003208\n",
      "3500: 0.014140\n",
      "3510: 0.005070\n",
      "3520: 0.006203\n",
      "3530: 0.006213\n",
      "3540: 0.005899\n",
      "3550: 0.006020\n",
      "3560: 0.007891\n",
      "3570: 0.009967\n",
      "3580: 0.007856\n",
      "3590: 0.008323\n",
      "3600: 0.006751\n",
      "3610: 0.004976\n",
      "3620: 0.004565\n",
      "3630: 0.014797\n",
      "3640: 0.005149\n",
      "3650: 0.003214\n",
      "3660: 0.003521\n",
      "3670: 0.004018\n",
      "3680: 0.009219\n",
      "3690: 0.003595\n",
      "3700: 0.002706\n",
      "3710: 0.003123\n",
      "3720: 0.002931\n",
      "3730: 0.002504\n",
      "3740: 0.002319\n",
      "3750: 0.008494\n",
      "3760: 0.004182\n",
      "3770: 0.004838\n",
      "3780: 0.003422\n",
      "3790: 0.003626\n",
      "3800: 0.003109\n",
      "3810: 0.016872\n",
      "3820: 0.009023\n",
      "3830: 0.006264\n",
      "3840: 0.004670\n",
      "3850: 0.003371\n",
      "3860: 0.002926\n",
      "3870: 0.002400\n",
      "3880: 0.002564\n",
      "3890: 0.002585\n",
      "3900: 0.004282\n",
      "3910: 0.010047\n",
      "3920: 0.003517\n",
      "3930: 0.004492\n",
      "3940: 0.003748\n",
      "3950: 0.002281\n",
      "3960: 0.002523\n",
      "3970: 0.002532\n",
      "3980: 0.002270\n",
      "3990: 0.002027\n",
      "4000: 0.001998\n",
      "4010: 0.001885\n",
      "4020: 0.001830\n",
      "4030: 0.001699\n",
      "4040: 0.001455\n",
      "4050: 0.001860\n",
      "4060: 0.001908\n",
      "4070: 0.001992\n",
      "4080: 0.002476\n",
      "4090: 0.002093\n",
      "4100: 0.002196\n",
      "4110: 0.002079\n",
      "4120: 0.001796\n",
      "4130: 0.001908\n",
      "4140: 0.002218\n",
      "4150: 0.002604\n",
      "4160: 0.001677\n",
      "4170: 0.003305\n",
      "4180: 0.002573\n",
      "4190: 0.002779\n",
      "4200: 0.008216\n",
      "4210: 0.002360\n",
      "4220: 0.002458\n",
      "4230: 0.001872\n",
      "4240: 0.004078\n",
      "4250: 0.002059\n",
      "4260: 0.001998\n",
      "4270: 0.002233\n",
      "4280: 0.002002\n",
      "4290: 0.002476\n",
      "4300: 0.001877\n",
      "4310: 0.002110\n",
      "4320: 0.001692\n",
      "4330: 0.001981\n",
      "4340: 0.002079\n",
      "4350: 0.001792\n",
      "4360: 0.002338\n",
      "4370: 0.002306\n",
      "4380: 0.001849\n",
      "4390: 0.010518\n",
      "4400: 0.016243\n",
      "4410: 0.009382\n",
      "4420: 0.004941\n",
      "4430: 0.003567\n",
      "4440: 0.003095\n",
      "4450: 0.002323\n",
      "4460: 0.002437\n",
      "4470: 0.004540\n",
      "4480: 0.001814\n",
      "4490: 0.001352\n",
      "4500: 0.001634\n",
      "4510: 0.001290\n",
      "4520: 0.001119\n",
      "4530: 0.003099\n",
      "4540: 0.001943\n",
      "4550: 0.001153\n",
      "4560: 0.001877\n",
      "4570: 0.001275\n",
      "4580: 0.001126\n",
      "4590: 0.001688\n",
      "4600: 0.001361\n",
      "4610: 0.001479\n",
      "4620: 0.002612\n",
      "4630: 0.001186\n",
      "4640: 0.003058\n",
      "4650: 0.001340\n",
      "4660: 0.001751\n",
      "4670: 0.003216\n",
      "4680: 0.002689\n",
      "4690: 0.001960\n",
      "4700: 0.002123\n",
      "4710: 0.002011\n",
      "4720: 0.002464\n",
      "4730: 0.003170\n",
      "4740: 0.003461\n",
      "4750: 0.002219\n",
      "4760: 0.001785\n",
      "4770: 0.001394\n",
      "4780: 0.001495\n",
      "4790: 0.001232\n",
      "4800: 0.002165\n",
      "4810: 0.004480\n",
      "4820: 0.002335\n",
      "4830: 0.001742\n",
      "4840: 0.001633\n",
      "4850: 0.002204\n",
      "4860: 0.001900\n",
      "4870: 0.001502\n",
      "4880: 0.001152\n",
      "4890: 0.002324\n",
      "4900: 0.013882\n",
      "4910: 0.003115\n",
      "4920: 0.002080\n",
      "4930: 0.001564\n",
      "4940: 0.001011\n",
      "4950: 0.001008\n",
      "4960: 0.001155\n",
      "4970: 0.002765\n",
      "4980: 0.005362\n",
      "4990: 0.004531\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train\n",
    "'''\n",
    "saver = tf.train.Saver()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_history = np.zeros((num_iters))\n",
    "verbose = True\n",
    "#plt.clf()\n",
    "for i in range(num_iters):\n",
    "    _x_batch = utils.next_minibatch(X_train, batch_size, data_aug=True)\n",
    "    x_in   = _x_batch[0]\n",
    "    x_true = _x_batch[1]\n",
    "    \n",
    "    if verbose:\n",
    "        error = sess.run(loss, feed_dict={X_input: x_in, X_truth: x_true})\n",
    "        loss_history[i] = error\n",
    "        if i % 10 == 0:\n",
    "            print('{}: {:.6f}'.format(i, error))\n",
    "    train.run(feed_dict={X_input: x_in, X_truth: x_true})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Model/Params/Wh_0:0' shape=(6, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_1:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_2:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_3:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_4:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_5:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_6:0' shape=(3, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_7:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_8:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_9:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_10:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_11:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_0/Adam:0' shape=(6, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_0/Adam_1:0' shape=(6, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_1/Adam:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_1/Adam_1:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_2/Adam:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_2/Adam_1:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_3/Adam:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_3/Adam_1:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_4/Adam:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_4/Adam_1:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_5/Adam:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_5/Adam_1:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_6/Adam:0' shape=(3, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_6/Adam_1:0' shape=(3, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_7/Adam:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_7/Adam_1:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_8/Adam:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_8/Adam_1:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_9/Adam:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_9/Adam_1:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_10/Adam:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_10/Adam_1:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_11/Adam:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Model/Params/Wh_11/Adam_1:0' shape=(8, 3) dtype=float32_ref>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-processing: nearest-neighbors and sparse adjacency\n",
    "'''\n",
    "\n",
    "'''\n",
    "SPARSE ADJACENCY MATRIX \n",
    "• scikit learn gives a CRS sparse adjacency for example, sparse TF takes COO. Use this for sparse X dense matmul in TF.\n",
    "• also return adjacency lists and convert alist into index list to be used for generic normalizations (avg, max, etc)\n",
    "\n",
    "tf has a great collection of sparse ops. Has a sparse/dense matmul, which may suit our needs. \n",
    "See which is faster/works:\n",
    "tf.sparse_tensor_dense_matmul\n",
    "tf.sparse_reduce\n",
    "some other combination of csr.indptr and reduceat functions (not sure tf.reduce_mean can take reduction_along indices)\n",
    "'''\n",
    "\n",
    "# returns adjacency lists based on NN in coordinate space\n",
    "def adjacency_list(X_in,k):\n",
    "    shape_in = X_in.shape\n",
    "    X_out = np.zeros([shape_in[0],shape_in[1],k],dtype=np.int32)\n",
    "    for b in range(shape_in[0]):\n",
    "        X_out[b] = kneighbors_graph(X_in[b,:,:3],k,include_self=True).indices.reshape([shape_in[1],k])\n",
    "    return X_out\n",
    "\n",
    "def get_adjacency_list(X_in,k):\n",
    "    \"\"\" search for k nneighbors, and return offsetted indices in adjacency list\n",
    "    \n",
    "    Args:\n",
    "        X_in: input data of shape (mb_size, N, 6)\n",
    "        k: number of nearest neighbors\n",
    "    \"\"\"\n",
    "    mb_size, N, D = X_in.shape\n",
    "    X_out = np.zeros([mb_size, N, k],dtype=np.int32)\n",
    "    for b in range(mb_size):\n",
    "        # this returns indices of the nn\n",
    "        graph_idx = kneighbors_graph(X_in[b,:,:3],k,include_self=True).indices.reshape([N,k]) + (N * b)\n",
    "        X_out[b] = graph_idx\n",
    "    return X_out\n",
    "\n",
    "# adjacency list to proper index list for get_item\n",
    "def alist_to_indexlist(alist):\n",
    "    \"\"\" tiles batch indices to adjacency list for tf.gather\n",
    "    \"\"\"\n",
    "    b, n, k = alist.shape\n",
    "    #b = alist.shape[0] # batch size\n",
    "    #n = alist.shape[1] # set size\n",
    "    #k = alist.shape[2] # number of nn\n",
    "    id1 = np.reshape(np.arange(b),[b,1])\n",
    "    id1 = np.tile(id1,n*k).flatten()\n",
    "    out = np.stack([id1,alist.flatten()],axis=1)\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

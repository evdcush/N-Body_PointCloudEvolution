{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, code, sys, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import cuda\n",
    "from random import randrange\n",
    "from numpy import linalg as LA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # IMPLEMENTATION NOTES\\n - network params: need to make it so param Variables are initialized with a fixed label,\\n   so that only one set of network hyper-params are defined in graph at a time\\n   (so tf.Variable(rand_init_values, name=LABEL.format(i)))\\n - make fwd and network functions\\n - been blackboxing Daniele's periodic boundary code, need to confirm that no extra space or extra loops being used\\n - see how framework-agnostic utils functions are. chainer.cuda.get_array_module should not be a problem if numpy data is sent\\n - compare sparse ops for density graph\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # IMPLEMENTATION NOTES\n",
    " - network params: need to make it so param Variables are initialized with a fixed label,\n",
    "   so that only one set of network hyper-params are defined in graph at a time\n",
    "   (so tf.Variable(rand_init_values, name=LABEL.format(i)))\n",
    " - make fwd and network functions\n",
    " - been blackboxing Daniele's periodic boundary code, need to confirm that no extra space or extra loops being used\n",
    " - see how framework-agnostic utils functions are. chainer.cuda.get_array_module should not be a problem if numpy data is sent\n",
    " - compare sparse ops for density graph\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Session, network settings\n",
    "'''\n",
    "params_seed = 98765\n",
    "data_seed   = 12345\n",
    "def seed_rng(s=data_seed):\n",
    "    np.random.seed(s)\n",
    "    tf.set_random_seed(s)\n",
    "    print('seeded by {}'.format(s))\n",
    "\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded by 12345\n",
      "Using redshifts z0.6, z0.0, with 4096 particles\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataset parameters\n",
    "'''\n",
    "num_particles = 16 # defaults 16**3\n",
    "zX = 0.6\n",
    "zY = 0.0\n",
    "#X_input, X_truth = utils.load_data(num_particles, zX, zY, normalize_data=True)\n",
    "rs_start = utils.REDSHIFTS.index(zX)\n",
    "rs_target = utils.REDSHIFTS.index(zY)\n",
    "X = utils.load_npy_data(num_particles) # (11, N, D, 6)\n",
    "X = X[[rs_start, rs_target]] # (2, N, D, 6)\n",
    "X = utils.normalize_fullrs(X)\n",
    "seed_rng()\n",
    "X_train, X_val = utils.multi_split_data_validation(X, num_val_samples=200)\n",
    "X = None # reduce memory overhead\n",
    "#X_input = np.load('X16_06.npy')\n",
    "#X_truth = np.load('X16_00.npy')\n",
    "print('Using redshifts z{}, z{}, with {} particles'.format(zX,zY,num_particles**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_H_LABEL     = 'Wh_{}'\n",
    "WEIGHT_GRAPH_LABEL = 'Wg_{}'\n",
    "BIAS_LABEL   = 'B_{}' # eg 'B_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model parameter initializations\n",
    "'''\n",
    "\n",
    "''' # earlier attempts, without scope, to be deleted\n",
    "def _init_weight(k_in, k_out, var_name):\n",
    "    \"\"\" Initialize weights for fully connected layer\n",
    "    weight drawn from he-normal distribution\n",
    "    Args:\n",
    "        k_in  (int): input channels\n",
    "        k_out (int): output channels\n",
    "    Returns: tf.Variable holding weight of shape (k_in, k_out)\n",
    "    \"\"\"\n",
    "    henorm_std = np.sqrt(2.0 / k_in)\n",
    "    weight = tf.random_normal((k_in, k_out), stddev=henorm_std)\n",
    "    return tf.Variable(weight, name=var_name)\n",
    "\n",
    "def _init_set_weights(k_in, k_out, layer_idx):\n",
    "    \"\"\" initializes weight for fully-connected layer\n",
    "    \"\"\"\n",
    "    Wh = init_weight(k_in, k_out, WEIGHT_H_LABEL.format(layer_idx))\n",
    "    return Wh    \n",
    "\n",
    "def _init_graph_weights(k_in, k_out, layer_idx):\n",
    "    \"\"\" initialize weights for graph layer\n",
    "    Two weights:\n",
    "        Wh : weight for external/hidden input (k_in, k_out)\n",
    "        Wg : weight for graph input (k_in, k_out)\n",
    "    \"\"\"\n",
    "    Wh = init_weight(k_in, k_out, WEIGHT_H_LABEL.format(layer_idx))\n",
    "    Wg = init_weight(k_in, k_out, WEIGHT_GRAPH_LABEL.format(layer_idx))\n",
    "    return Wh, Wg\n",
    "\n",
    "def _init_bias(k_in, k_out, layer_idx):\n",
    "    \"\"\" initalize bias param\n",
    "    Bias initialized to be near zero\n",
    "    Returns: tf.Variable of shape (k_out,) for bias\n",
    "    \"\"\"\n",
    "    bias = np.ones(k_out).astype(np.float32) * 1e-6\n",
    "    return tf.Variable(bias, BIAS_LABEL.format(layer_idx))\n",
    "\n",
    "def _init_params(channels, graph_weights=True, use_bias=False):\n",
    "    \"\"\" initializes all network hyperparameters\n",
    "    Creates a dict with weights and biases associated with each\n",
    "    hidden layer\n",
    "    Args:\n",
    "        channels (list): list of channel sizes\n",
    "        graph_weights: if true, initializes weights for graph model\n",
    "        use_bias: if true, bias params initialized, else None\n",
    "    Returns: params dict containing weight and biases\n",
    "    \"\"\"\n",
    "    weight_init_fun = init_graph_weights if graph_weights else init_set_weights\n",
    "    kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]    \n",
    "    weights = []\n",
    "    biases  = [] if use_bias else None\n",
    "    for idx, ktup in enumerate(kdims):\n",
    "        weights.append(weight_init_fun(*ktup, idx))     \n",
    "        if use_bias: biases.append(init_bias(*ktup, idx))\n",
    "    params = {'Weights': weights, 'Biases': biases}\n",
    "    return params\n",
    "'''\n",
    "dont_print_cell = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "using tf scope, get\n",
    "'''\n",
    "def init_weight(k_in, k_out, var_name):\n",
    "    \"\"\" Initialize weights for fully connected layer\n",
    "    weight drawn from glorot normal distribution\n",
    "    Args:\n",
    "        k_in  (int): input channels\n",
    "        k_out (int): output channels\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Params\", reuse=tf.AUTO_REUSE):\n",
    "        tf.get_variable(var_name, shape=(k_in, k_out), initializer=tf.glorot_normal_initializer())\n",
    "\n",
    "def init_bias(k_in, k_out, layer_idx):\n",
    "    \"\"\" initalize bias param\n",
    "    Bias initialized to be near zero# actually zero for now\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"Params\", reuse=tf.AUTO_REUSE):\n",
    "        # should be init with values near 0\n",
    "        tf.get_variable(var_name, shape=(k_out,), initializer=tf.zeros_initializer())\n",
    "\n",
    "def init_gmodel_params(channels, use_bias=False):\n",
    "    kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]\n",
    "    for idx, ktup in enumerate(kdims):\n",
    "        # init external/hidden weights\n",
    "        wh_name = WEIGHT_H_LABEL.format(idx)\n",
    "        init_weight(*ktup, wh_name)\n",
    "        # init graph weights\n",
    "        wg_name = WEIGHT_GRAPH_LABEL.format(idx)        \n",
    "        init_weight(*ktup, wg_name)\n",
    "        # bias\n",
    "        if use_bias:\n",
    "            b_name = BIAS_LABEL.format(idx)\n",
    "            init_bias(*ktup, b_name)\n",
    "\n",
    "def init_model_params(channels, use_bias=False):\n",
    "    kdims = [(channels[i], channels[i+1]) for i in range(len(channels) - 1)]\n",
    "    with tf.variable_scope(\"Model\", reuse=tf.AUTO_REUSE):\n",
    "        for idx, ktup in enumerate(kdims):\n",
    "            # init external/hidden weights\n",
    "            wh_name = WEIGHT_H_LABEL.format(idx)\n",
    "            init_weight(*ktup, wh_name)\n",
    "            # bias\n",
    "            if use_bias:\n",
    "                b_name = BIAS_LABEL.format(idx)\n",
    "                init_bias(*ktup, b_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nn ops\n",
    "'''\n",
    "def left_mult(h, w):\n",
    "    return tf.einsum('ijl,lq->ijq', h, w)\n",
    "\n",
    "def linear_fwd(h_in, layer_idx, use_bias):\n",
    "    with tf.variable_scope(\"Params\", reuse=tf.AUTO_REUSE):\n",
    "        weight = tf.get_variable(WEIGHT_H_LABEL.format(layer_idx))\n",
    "        mean   = tf.reduce_mean(h_in, axis=-1, keepdims=True)\n",
    "        h = h_in - mean\n",
    "        h_out = left_mult(h, weight)\n",
    "        if use_bias:\n",
    "            bias = tf.get_variable(BIAS_LABEL.format(layer_idx))\n",
    "            h_out += bias\n",
    "    return h_out\n",
    "\n",
    "def model_fwd(x_in, num_layers, activation=tf.nn.relu, use_bias=False):\n",
    "    with tf.variable_scope(\"Model\", reuse=tf.AUTO_REUSE):\n",
    "        h = x_in\n",
    "        for i in range(num_layers):\n",
    "            #print('model_fwd: {}'.format(i))\n",
    "            h = linear_fwd(h, i, use_bias)\n",
    "            if i != num_layers - 1:\n",
    "                h = activation(h)\n",
    "    return h\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "def get_readout(h_out):\n",
    "    gt_one  = (tf.sign(h_out - 1) + 1) / 2\n",
    "    ls_zero = -(tf.sign(h_out) - 1) / 2\n",
    "    rest = 1 - gt_one - ls_zero\n",
    "    readout = rest*h_out + gt_one*(h_out - 1) + ls_zero*(1 + h_out)\n",
    "    return readout\n",
    "\n",
    "def periodic_boundary_dist(readout, x_truth):\n",
    "    x_truth_coo = x_truth[...,:3]\n",
    "    dist = tf.minimum(tf.square(readout - x_truth_coo), tf.square(readout - (1 + x_truth_coo)))\n",
    "    dist = tf.minimum(dist, tf.square((1 + readout) - x_truth_coo))\n",
    "    return dist\n",
    "    \n",
    "def pbc_loss(h_out, x_truth):\n",
    "    readout  = get_readout(h_out)\n",
    "    pbc_dist = periodic_boundary_dist(readout, x_truth)\n",
    "    pbc_error = tf.reduce_mean(tf.reduce_sum(pbc_dist, axis=-1))\n",
    "    return pbc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded by 98765\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model\n",
    "'''\n",
    "channels = [6, 8, 16, 32, 16, 8, 3, 8, 16, 32, 16, 8, 3]\n",
    "num_layers = len(channels) - 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 8\n",
    "num_iters = 10\n",
    "use_bias = False\n",
    "seed_rng(params_seed)\n",
    "#init_graph_params(channels)\n",
    "init_model_params(channels, use_bias=use_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_in   = X_train[0]\n",
    "#X_true = X_train[1]\n",
    "#x_input = tf.constant(X_in[:8])\n",
    "#x_truth = tf.constant(X_true[:8])\n",
    "#x_hat = model_fwd(x_input, num_layers, use_bias=use_bias)\n",
    "#loss = pbc_loss(x_hat, x_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "#sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizer\n",
    "'''\n",
    "X_input = tf.placeholder(tf.float32, shape=[None,num_particles**3, channels[0]], name='X_input')\n",
    "X_truth = tf.placeholder(tf.float32, shape=[None,num_particles**3, channels[0]], name='X_truth')\n",
    "X_hat   = model_fwd(X_input, num_layers, use_bias=use_bias)\n",
    "\n",
    "loss    = pbc_loss(X_hat, X_truth)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.251520\n",
      "1: 0.254670\n",
      "2: 0.255700\n",
      "3: 0.247222\n",
      "4: 0.256667\n",
      "5: 0.257706\n",
      "6: 0.249838\n",
      "7: 0.242250\n",
      "8: 0.250912\n",
      "9: 0.250447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd47d494160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Train\n",
    "'''\n",
    "saver = tf.train.Saver()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_history = np.zeros((num_iters))\n",
    "verbose = True\n",
    "#plt.clf()\n",
    "for i in range(num_iters):\n",
    "    _x_batch = utils.next_minibatch(X_train, batch_size, data_aug=True)\n",
    "    x_in   = _x_batch[0]\n",
    "    x_true = _x_batch[1]\n",
    "    \n",
    "    if verbose:\n",
    "        error = sess.run(loss, feed_dict={X_input: x_in, X_truth: x_true})\n",
    "        loss_history[i] = error\n",
    "        print('{}: {:.6f}'.format(i, error))\n",
    "    train.run(feed_dict={X_input: x_in, X_truth: x_true})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in   = X_input[:8]\n",
    "x_true = X_truth[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xin   = tf.constant(x_in)\n",
    "Xtrue = tf.constant(x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-496211573949>:10: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "pred = model_fwd(Xin, num_layers, use_bias=use_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = pred.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65673"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98304"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(x_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of a new variable (Model/Params/Wh_0) must be fully defined, but instead was <unknown>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-83a079bf89a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-8a7204c28588>\u001b[0m in \u001b[0;36mmodel_fwd\u001b[0;34m(X_in, num_layers, activation, use_bias)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8a7204c28588>\u001b[0m in \u001b[0;36mlinear_fwd\u001b[0;34m(h_in, layer_idx, use_bias)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWEIGHT_H_LABEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmean\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_in\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/C/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1263\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1264\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/C/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1095\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/C/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/C/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/C/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n\u001b[0;32m--> 764\u001b[0;31m                        \"but instead was %s.\" % (name, shape))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;31m# Create the tensor to initialize the variable with default value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of a new variable (Model/Params/Wh_0) must be fully defined, but instead was <unknown>."
     ]
    }
   ],
   "source": [
    "pred = sess.run(model_fwd(Xin, len(channels)-1, use_bias=use_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(8, 4096, 6) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(8, 4096, 6) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Weights/Wh_0:0' shape=(6, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_0:0' shape=(6, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_1:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_1:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_2:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_2:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_3:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_3:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_4:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_4:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_5:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_5:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_6:0' shape=(3, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_6:0' shape=(3, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_7:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_7:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_8:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_8:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_9:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_9:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_10:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_10:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wh_11:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Weights/Wg_11:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_0:0' shape=(6, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_1:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_2:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_3:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_4:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_5:0' shape=(8, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_6:0' shape=(3, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_7:0' shape=(8, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_8:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_9:0' shape=(32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_10:0' shape=(16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'Set/Weights/Wh_11:0' shape=(8, 3) dtype=float32_ref>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Set/Weights\", reuse=tf.AUTO_REUSE):\n",
    "    foo = tf.get_variable('Wh_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Set/Weights/Wh_8:0' shape=(16, 32) dtype=float32_ref>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Network layers\n",
    "'''\n",
    "\n",
    "def left_mult(T1,T2):\n",
    "    Tout = tf.einsum('ijl,lq->ijq',T1,T2)\n",
    "    return Tout\n",
    "def linear_fwd(h, layer_idx):\n",
    "    with tf.variable_scope(\"Weights\", reuse=tf.AUTO_REUSE):\n",
    "        weight = tf.get_variable(WEIGHT_H_LABEL.format(layer_idx))\n",
    "    return tf.einsum(h, weight)\n",
    "\n",
    "def model_fwd(X_in, num_layers, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(\"Model\", reuse=tf.AUTO_REUSE):\n",
    "        h = x_in\n",
    "        for i in range(num_layers):\n",
    "            h = linear_fwd(h, i)\n",
    "            if i != num_layers - 1:\n",
    "                h = activation(h)\n",
    "    return h            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-processing: nearest-neighbors and sparse adjacency\n",
    "'''\n",
    "\n",
    "'''\n",
    "SPARSE ADJACENCY MATRIX \n",
    "• scikit learn gives a CRS sparse adjacency for example, sparse TF takes COO. Use this for sparse X dense matmul in TF.\n",
    "• also return adjacency lists and convert alist into index list to be used for generic normalizations (avg, max, etc)\n",
    "\n",
    "tf has a great collection of sparse ops. Has a sparse/dense matmul, which may suit our needs. \n",
    "See which is faster/works:\n",
    "tf.sparse_tensor_dense_matmul\n",
    "tf.sparse_reduce\n",
    "some other combination of csr.indptr and reduceat functions (not sure tf.reduce_mean can take reduction_along indices)\n",
    "'''\n",
    "\n",
    "# returns adjacency lists based on NN in coordinate space\n",
    "def adjacency_list(X_in,k):\n",
    "    shape_in = X_in.shape\n",
    "    X_out = np.zeros([shape_in[0],shape_in[1],k],dtype=np.int32)\n",
    "    for b in range(shape_in[0]):\n",
    "        X_out[b] = kneighbors_graph(X_in[b,:,:3],k,include_self=True).indices.reshape([shape_in[1],k])\n",
    "    return X_out\n",
    "\n",
    "def get_adjacency_list(X_in,k):\n",
    "    \"\"\" search for k nneighbors, and return offsetted indices in adjacency list\n",
    "    \n",
    "    Args:\n",
    "        X_in: input data of shape (mb_size, N, 6)\n",
    "        k: number of nearest neighbors\n",
    "    \"\"\"\n",
    "    mb_size, N, D = X_in.shape\n",
    "    X_out = np.zeros([mb_size, N, k],dtype=np.int32)\n",
    "    for b in range(mb_size):\n",
    "        # this returns indices of the nn\n",
    "        graph_idx = kneighbors_graph(X_in[b,:,:3],k,include_self=True).indices.reshape([N,k]) + (N * b)\n",
    "        X_out[b] = graph_idx\n",
    "    return X_out\n",
    "\n",
    "# adjacency list to proper index list for get_item\n",
    "def alist_to_indexlist(alist):\n",
    "    \"\"\" tiles batch indices to adjacency list for tf.gather\n",
    "    \"\"\"\n",
    "    b, n, k = alist.shape\n",
    "    #b = alist.shape[0] # batch size\n",
    "    #n = alist.shape[1] # set size\n",
    "    #k = alist.shape[2] # number of nn\n",
    "    id1 = np.reshape(np.arange(b),[b,1])\n",
    "    id1 = np.tile(id1,n*k).flatten()\n",
    "    out = np.stack([id1,alist.flatten()],axis=1)\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
